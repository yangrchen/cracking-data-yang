<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.555">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Yang Chen">
<meta name="dcterms.date" content="2024-06-22">

<title>Cracking Data with Yang - Exploring Karpathy’s Makemore Concepts Using Fantasy DnD Names: Part 1</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Cracking Data with Yang</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/yangrchen"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Exploring Karpathy’s Makemore Concepts Using Fantasy DnD Names: Part 1</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">project</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Yang Chen </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">June 22, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#scraping-fantasy-names" id="toc-scraping-fantasy-names" class="nav-link" data-scroll-target="#scraping-fantasy-names">Scraping Fantasy Names</a></li>
  <li><a href="#bigram-language-model" id="toc-bigram-language-model" class="nav-link" data-scroll-target="#bigram-language-model">Bigram Language Model</a>
  <ul class="collapse">
  <li><a href="#generating-characters-through-sampling" id="toc-generating-characters-through-sampling" class="nav-link" data-scroll-target="#generating-characters-through-sampling">Generating Characters Through Sampling</a></li>
  </ul></li>
  <li><a href="#maximizing-the-log-likelihood-minimizing-the-negative-log-likelihood" id="toc-maximizing-the-log-likelihood-minimizing-the-negative-log-likelihood" class="nav-link" data-scroll-target="#maximizing-the-log-likelihood-minimizing-the-negative-log-likelihood">Maximizing the Log-Likelihood (Minimizing the Negative Log-Likelihood)</a></li>
  <li><a href="#trigram-model" id="toc-trigram-model" class="nav-link" data-scroll-target="#trigram-model">Trigram Model</a>
  <ul class="collapse">
  <li><a href="#generating-characters-for-the-trigram-model" id="toc-generating-characters-for-the-trigram-model" class="nav-link" data-scroll-target="#generating-characters-for-the-trigram-model">Generating Characters for the Trigram Model</a></li>
  </ul></li>
  <li><a href="#neural-net---collectors-edition" id="toc-neural-net---collectors-edition" class="nav-link" data-scroll-target="#neural-net---collectors-edition">Neural Net - Collector’s Edition</a>
  <ul class="collapse">
  <li><a href="#bigram-model" id="toc-bigram-model" class="nav-link" data-scroll-target="#bigram-model">Bigram Model</a></li>
  <li><a href="#optimizing-the-neural-net" id="toc-optimizing-the-neural-net" class="nav-link" data-scroll-target="#optimizing-the-neural-net">Optimizing the Neural Net</a>
  <ul class="collapse">
  <li><a href="#training-loop" id="toc-training-loop" class="nav-link" data-scroll-target="#training-loop">Training Loop</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#trigram-neural-net-optimizing-code" id="toc-trigram-neural-net-optimizing-code" class="nav-link" data-scroll-target="#trigram-neural-net-optimizing-code">Trigram Neural Net, Optimizing Code</a>
  <ul class="collapse">
  <li><a href="#one-hot-encoding-seems-wasteful" id="toc-one-hot-encoding-seems-wasteful" class="nav-link" data-scroll-target="#one-hot-encoding-seems-wasteful">One-Hot Encoding Seems Wasteful…</a></li>
  <li><a href="#cross-entropy-loss" id="toc-cross-entropy-loss" class="nav-link" data-scroll-target="#cross-entropy-loss">Cross Entropy Loss</a></li>
  <li><a href="#trigram-model-1" id="toc-trigram-model-1" class="nav-link" data-scroll-target="#trigram-model-1">Trigram Model</a></li>
  <li><a href="#generating-characters-from-neural-net" id="toc-generating-characters-from-neural-net" class="nav-link" data-scroll-target="#generating-characters-from-neural-net">Generating Characters from Neural Net</a></li>
  </ul></li>
  <li><a href="#appendix" id="toc-appendix" class="nav-link" data-scroll-target="#appendix">Appendix</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/yangrchen/cracking-data-yang/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>I dedicated a month to understanding the mechanics of language modeling—without too much of the marketing hype attached with the popularity of large language models. During my studies, I came across Andrej Karpathy’s <a href="https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">makemore video series</a>. He progressively builds up the concepts and code that lead to generative pre-training transformer (GPT) models starting with character-level language models.</p>
<p>To say the least, this video series is a remarkably detailed set of lectures that really help demystify language modeling all the way up through deep learning concepts. I wanted to try my hand at applying this awesome work on a fun dataset that I’ve curated myself—DnD fantasy names (the amazing drow)! I like Karpathy’s advice on “creating cheatsheets” too, so this post is my personal (large) cheatsheet that hopefully helps beginners like me on their own journey.</p>
<p>This project will be broken up into 4 parts:</p>
<ul>
<li><strong>Part 1</strong> will cover scraping the necessary data, creating a counting bigram model, understanding negative log-likelihood loss, and scaling up the model as a neural net.</li>
<li><strong>Part 2</strong> is all about using embeddings and creating a regular feedforward neural net to generate names. We will create our train/val/test split and experiment with some settings to improve our performance.</li>
<li><strong>Part 3</strong> will expand on the feedforward net from Part 2, diving into better weight initializations and BatchNorm to stabilize the model’s performance.</li>
<li><strong>Part 4</strong> is TBD! But I’d like to build a transformer on fantasy dialogue.</li>
</ul>
</section>
<section id="scraping-fantasy-names" class="level1">
<h1>Scraping Fantasy Names</h1>
<p>In the makemore series, Karpathy uses a text file of English names. I’m putting a bit of a fantasy spin on it by collecting names from <a href="https://www.fantasynamegenerators.com/dnd-drow-names.php">Fantasy Name Generators - Drow Names</a>. To do this, I created a script to repeatedly play cookie clicker by clicking a button on the page and grabbing the names from the results section that pops up on the site.</p>
<p>The cell below writes a new file with the code into the working directory.</p>
<div id="cell-3" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>writefile scrape.py</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dotenv <span class="im">import</span> load_dotenv</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> selenium <span class="im">import</span> webdriver</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> selenium.webdriver.common.by <span class="im">import</span> By</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> selenium.webdriver.chrome.webdriver <span class="im">import</span> WebDriver</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> selenium.webdriver.chrome.service <span class="im">import</span> Service</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> selenium.webdriver.chrome.options <span class="im">import</span> Options</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> selenium.webdriver.support.ui <span class="im">import</span> WebDriverWait</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> selenium.webdriver.support <span class="im">import</span> expected_conditions <span class="im">as</span> EC</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>load_dotenv()</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> click_and_extract(</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    driver: WebDriver, button_selector: <span class="bu">str</span>, div_selector: <span class="bu">str</span>, duration<span class="op">=</span><span class="dv">60</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Repeatedly clicks and extracts text from the fantasy name generator for a specified duration.</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co">    Website used can be found at: https://www.fantasynamegenerators.com/dnd-drow-names.php.</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    names <span class="op">=</span> <span class="st">""</span>  <span class="co"># Maintain a list of names to save</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    end_time <span class="op">=</span> time.time() <span class="op">+</span> duration</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> time.time() <span class="op">&lt;</span> end_time:</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Waits for a max of 10 sec. until the button element being looked for is found</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>            button <span class="op">=</span> WebDriverWait(driver, <span class="dv">10</span>).until(</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>                EC.presence_of_element_located((By.CSS_SELECTOR, button_selector))</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>            button.click()</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>            div <span class="op">=</span> WebDriverWait(driver, <span class="dv">5</span>).until(</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>                EC.presence_of_element_located((By.CSS_SELECTOR, div_selector))</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>            names <span class="op">+=</span> div.text.lower() <span class="op">+</span> <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>  <span class="co"># Added newline char to end of string to separate each section</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"An error occurred: </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> names</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>    chrome_options <span class="op">=</span> Options()</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>    service <span class="op">=</span> Service(os.getenv(<span class="st">"CHROMEDRIVER_PATH"</span>))  <span class="co"># Use the chrome driver on system</span></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>    driver <span class="op">=</span> webdriver.Chrome(service<span class="op">=</span>service, options<span class="op">=</span>chrome_options)</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>    url <span class="op">=</span> <span class="st">"https://www.fantasynamegenerators.com/dnd-drow-names.php"</span></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>    driver.get(url)</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>    button_selector <span class="op">=</span> (</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>        <span class="st">'#nameGen &gt; input[type="button"]'</span>  <span class="co"># Specific button selector for the website</span></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>    div_selector <span class="op">=</span> <span class="st">"#result"</span>  <span class="co"># Specific div selector for the generated results</span></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>    names <span class="op">=</span> click_and_extract(driver, button_selector, div_selector, duration<span class="op">=</span><span class="dv">120</span>)</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Write names to a text file</span></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(<span class="st">"names.txt"</span>, <span class="st">"w"</span>) <span class="im">as</span> f:</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>        f.write(names)</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Close driver connection</span></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>    driver.quit()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Overwriting scrape.py</code></pre>
</div>
</div>
<p>With some names in hand, we can explore the data some to see what we’re working with.</p>
<div id="cell-5" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Read in the names dataset as a list</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">"names.txt"</span>, <span class="st">"r"</span>) <span class="im">as</span> f:</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    names <span class="op">=</span> f.read().splitlines()</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(names))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>57880</code></pre>
</div>
</div>
<p>In <a href="#fig-name-freq" class="quarto-xref">Figure&nbsp;1</a>, we can see that there are multiple names that are duplicated throughout the dataset. For this project, we’ll only be using unique names and can easily filter out the repeated ones.</p>
<div id="cell-fig-name-freq" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>name_counts <span class="op">=</span> Counter(names)  <span class="co"># Get the frequency of every name</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>name_counts <span class="op">=</span> <span class="bu">sorted</span>(</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    name_counts.items(), key<span class="op">=</span><span class="kw">lambda</span> x: x[<span class="dv">1</span>], reverse<span class="op">=</span><span class="va">True</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>)  <span class="co"># Sort by descending count</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>plt.xticks(rotation<span class="op">=</span><span class="st">"vertical"</span>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>plt.bar([n[<span class="dv">0</span>] <span class="cf">for</span> n <span class="kw">in</span> name_counts][:<span class="dv">10</span>], [n[<span class="dv">1</span>] <span class="cf">for</span> n <span class="kw">in</span> name_counts][:<span class="dv">10</span>])<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-name-freq" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-name-freq-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-name-freq-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-name-freq-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: A sample bar chart of the most frequent names
</figcaption>
</figure>
</div>
</div>
</div>
<div id="cell-8" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>names <span class="op">=</span> <span class="bu">list</span>(<span class="bu">set</span>(names))  <span class="co"># Retrieve unique names</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">"names_clean.txt"</span>, <span class="st">"w"</span>) <span class="im">as</span> f:</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    f.write(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span>.join(names))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="bigram-language-model" class="level1">
<h1>Bigram Language Model</h1>
<p>The first model we can use as a potential baseline is a bigram model. With a bigram model, we will predict a new character based off the previous one. We will also introduce a <code>.</code> token to denote the start and end of a string. When generating a new word from scratch, a starting <code>.</code> will be used. For an example, where <code>-&gt;</code> will represent “leads to”:</p>
<ul>
<li>In the string <code>abcd</code>, <code>. -&gt; a</code>, <code>.a -&gt; b</code>, <code>ab -&gt; c</code>, <code>bc -&gt; d</code>, <code>cd -&gt; .</code></li>
</ul>
<p>The main difference in our dataset from the makemore one is the addition of a blank space (” “) token and an apostrophe (” ’ “).</p>
<div id="cell-10" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> {}</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, n <span class="kw">in</span> <span class="bu">enumerate</span>(names):</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    chs <span class="op">=</span> [<span class="st">"."</span>] <span class="op">+</span> <span class="bu">list</span>(n) <span class="op">+</span> [<span class="st">"."</span>]</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> ch1, ch2 <span class="kw">in</span> <span class="bu">zip</span>(chs, chs[<span class="dv">1</span>:]):</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>        bigram <span class="op">=</span> (ch1, ch2)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>        b[bigram] <span class="op">=</span> b.get(bigram, <span class="dv">0</span>) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Print the first two name examples</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">&lt;</span> <span class="dv">2</span>:</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(ch1, ch2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>. k
k r
r e
e k
k i
i i
i r
r  
  u
u l
l u
u a
a n
n .
. l
l l
l t
t y
y l
l n
n  
  t
t o
o r
r a
a t
t e
e .</code></pre>
</div>
</div>
<div id="cell-11" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Find the most likely bigrams by frequency</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="bu">sorted</span>(b.items(), key<span class="op">=</span><span class="kw">lambda</span> x: x[<span class="dv">1</span>], reverse<span class="op">=</span><span class="va">True</span>)[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>(('a', 'r'), 21057)</code></pre>
</div>
</div>
<p>By collecting the bigram frequencies, we can see which characters are most likely to follow other characters. For example, an <code>a</code> is most likely to be followed by an <code>r</code> based on the data we have. So we should be able to create a probability distribution out of these frequencies and sample from it to create new names!</p>
<p>Also, these models can’t process text directly so we need some way to encode our text numerically to be able to represent the information. When generating new characters at the inference stage, we can also have a decoder to translate the numbers back to text. Makemore indexes the characters in alphabetical order and assigns <code>.</code> to be the 0 index. We can do the same here.</p>
<div id="cell-13" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>chars <span class="op">=</span> <span class="bu">sorted</span>(<span class="bu">list</span>(<span class="bu">set</span>(<span class="st">""</span>.join(names))))  <span class="co"># Create the unique vocab set</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>stoi <span class="op">=</span> {</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    s: i <span class="cf">for</span> i, s <span class="kw">in</span> <span class="bu">enumerate</span>(chars, start<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>}  <span class="co"># Encode the characters by their alphabetical index, starting at index 1</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>stoi[<span class="st">"."</span>] <span class="op">=</span> <span class="dv">0</span>  <span class="co"># Assign the terminating token to index 0</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>itos <span class="op">=</span> {i: s <span class="cf">for</span> s, i <span class="kw">in</span> stoi.items()}  <span class="co"># Decode the indices to text</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>vocab_size <span class="op">=</span> <span class="bu">len</span>(stoi)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(vocab_size)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>29</code></pre>
</div>
</div>
<p>Throughout this project, we will take advantage of PyTorch for its deep learning and general computing capabilities. Instead of representing our bigram frequencies as a list of character pairs and their frequency, we can store this information as a <code>29x29</code> tensor based off our <code>vocab_size</code>.</p>
<p>How can we interpret this tensor? Based off our encoding, each row index will represent the first character in a bigram and each column index will represent the second character. Each element in <code>[row, col]</code> will be the frequency of that bigram.</p>
<div id="cell-15" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>itos[<span class="dv">17</span>], itos[<span class="dv">13</span>], <span class="st">"!"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>('o', 'k', '!')</code></pre>
</div>
</div>
<div id="cell-16" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> torch.zeros(</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    (vocab_size, vocab_size), dtype<span class="op">=</span>torch.int32</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>)  <span class="co"># Initializating the bigram frequency tensor</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-17" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Count the frequency of each bigram and store it in the tensor</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n <span class="kw">in</span> names:</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    chs <span class="op">=</span> [<span class="st">"."</span>] <span class="op">+</span> <span class="bu">list</span>(n) <span class="op">+</span> [<span class="st">"."</span>]</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> ch1, ch2 <span class="kw">in</span> <span class="bu">zip</span>(chs, chs[<span class="dv">1</span>:]):</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>        idx1, idx2 <span class="op">=</span> stoi[ch1], stoi[ch2]</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>        N[idx1, idx2] <span class="op">+=</span> <span class="dv">1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can also visualize our tensor in a fun way. You might notice that we have some sparse entries. There is only 1 bigram that occurs where <code>q</code> is the first character—<code>qu</code>. When it comes to generating names, if a <code>q</code> appears then the only character that can appear next is <code>u</code>. Now, we might be okay with this, but later when we come to evaluating this model we’ll find that this gives us a property that might not like that we’ll have to deal with.</p>
<div id="cell-19" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">16</span>, <span class="dv">16</span>))</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>plt.imshow(N, cmap<span class="op">=</span><span class="st">"Blues"</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(vocab_size):</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(vocab_size):</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>        chstr <span class="op">=</span> itos[i] <span class="op">+</span> itos[j]</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>        plt.text(j, i, chstr, ha<span class="op">=</span><span class="st">"center"</span>, va<span class="op">=</span><span class="st">"bottom"</span>, color<span class="op">=</span><span class="st">"gray"</span>)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>        plt.text(j, i, N[i, j].item(), ha<span class="op">=</span><span class="st">"center"</span>, va<span class="op">=</span><span class="st">"top"</span>, color<span class="op">=</span><span class="st">"gray"</span>)</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">"off"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-12-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<section id="generating-characters-through-sampling" class="level2">
<h2 class="anchored" data-anchor-id="generating-characters-through-sampling">Generating Characters Through Sampling</h2>
<p>With our frequencies in place, we can start generating names! Intuitively, row 0 represents the frequencies of every other character following the <code>.</code> character. So if we want to find the probability of each character following <code>.</code> then we just have to divide each row entry by the sum of all the row frequencies. We now have a probability distribution for characters following <code>.</code></p>
<p>We’ll do the same for every other row and <strong>SHAZAM</strong>… we have a probability matrix we can sample from.</p>
<div id="cell-22" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>P <span class="op">=</span> N.<span class="bu">float</span>()</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>P <span class="op">/=</span> P.<span class="bu">sum</span>(</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    dim<span class="op">=</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>)  <span class="co"># By keeping the row dimension, the row sum array will be broadcasted across the rest of P</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> torch.allclose(</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>    torch.ones(P.shape[<span class="dv">0</span>], dtype<span class="op">=</span>torch.float32), P.<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>)  <span class="co"># Check to see that every row sums to 1 in our probability matrix</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-23" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">1234</span>)  <span class="co"># Reproducibility</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> []</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>    ix <span class="op">=</span> <span class="dv">0</span>  <span class="co"># Start with the `.` character</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>        p <span class="op">=</span> P[ix]</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>        ix <span class="op">=</span> torch.multinomial(</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>            p, num_samples<span class="op">=</span><span class="dv">1</span>, replacement<span class="op">=</span><span class="va">True</span>, generator<span class="op">=</span>g</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>        ).item()  <span class="co"># Sample from our probability matrix and get back an index which represents the new character</span></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> ix <span class="op">==</span> <span class="dv">0</span>:  <span class="co"># Terminate if we end up back on the `.` char</span></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>        out.append(itos[ix])  <span class="co"># Append the decoded character to our output</span></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">""</span>.join(out))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>irorelelatodele sonzmr
sd
rn kho'gorlavzae
h drinan s
clorhimllan dhelolaarlahimanin aun mtathan
ileryn jhyol'tont
thes cenodaelar zabh
oltenn zalarlyr xl'lan drar alrathurilaly
zamvadrylmdal zekrazarifar
duendalaurrilelyaeanina'auicod</code></pre>
</div>
</div>
<p>I’d love to see a DnD character with the name <code>duendalaurrilelyaeanina'auicod</code>. Unfortunately, I don’t know any that exist yet. There are definitely some names that sound like they <em>could</em> be in DnD like <code>ileryn</code> and <code>zekrazarifar</code>, but also we can see some strange artifacts like <code>sd</code> and <code>rn</code> hanging around.</p>
<p>Compared to typical English names, it can be difficult to verify how good the output is or even if the bigram model is working as expected. One sanity check is to make every character have uniform probability—every character is equally likely to be sampled.</p>
<div id="cell-25" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">1234</span>)  <span class="co"># Reproducibility</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> []</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>    ix <span class="op">=</span> <span class="dv">0</span>  <span class="co"># Start with the `.` character</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># p = P[ix]</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>        p <span class="op">=</span> torch.ones(vocab_size) <span class="op">/</span> vocab_size  <span class="co"># Uniform distribution</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>        ix <span class="op">=</span> torch.multinomial(</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>            p, num_samples<span class="op">=</span><span class="dv">1</span>, replacement<span class="op">=</span><span class="va">True</span>, generator<span class="op">=</span>g</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>        ).item()  <span class="co"># Sample from our probability matrix and get back an index which represents the new character</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> ix <span class="op">==</span> <span class="dv">0</span>:  <span class="co"># Terminate if we end up back on the `.` char</span></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>        out.append(itos[ix])  <span class="co"># Append the decoded character to our output</span></span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">""</span>.join(out))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>wxoorlbqjaodxlfwsoizmjzsd'rlovhj'govlnvzqpchkdlijvfqswc
oyhimsljayrhnjxlaawlmhifanicgempzuhatngychlmwffvjdm'u'fkdsftyfsdc
suplhfqkrzwpfpxlf'ezpz'lqrlpfnxc'cqqcdbac airmqhugilnkr
zbmvpdrtbmdyf ze raw'ipfo xduccyaqrimwuieqwpeaosca'ndiccd
'tzbyl lyhzbzchnfqtjdcwzyrhy q klmsod yafxwcbwziqu'mtdhhycrjpmjdmhcnowtcdzffadoebpodazgqqjswqudquwanz'ncaktcxk
mqulrvhoez
kupksskhynn
l efaqjvcrpljcxqn
rayi
z</code></pre>
</div>
</div>
<p>All hail the demonic <code>wxoorlbqjaodxlfwsoizmjzsd'rlovhj'govlnvzqpchkdlijvfqswc</code>. Your soul will be split in half just trying to pronounce that. It definitely seems like the bigram model is working to an extent.</p>
<p>However, we can still have a bit more confidence by having a method to summarize our model numerically.</p>
</section>
</section>
<section id="maximizing-the-log-likelihood-minimizing-the-negative-log-likelihood" class="level1">
<h1>Maximizing the Log-Likelihood (Minimizing the Negative Log-Likelihood)</h1>
<p><strong>Likelihood</strong> refers to the probability of observing our data under a specific model and set of model parameters. In the context of model optimization, we are trying to optimize our parameter values that will maximize some likelihood function. For each independent character class, we multiply our probabilities of predicting that character together to get our likelihood.</p>
<p>However, in practice we will use <strong>log-likelihood</strong>, particular for three reasons:</p>
<ul>
<li>Log-likelihood is a <strong>monotonic</strong> transformation. Maximizing the log-likelihood also maximizes the original likelihood.</li>
<li>Sums are easier to work with than products. Mathematically, by applying <span class="math inline">\(\log\)</span> to a product we can represent it as a summation instead: <span class="math inline">\(\log(a \cdot b \cdot c) = \log(a) + \log(b) + \log(c)\)</span>. Eventually, when it comes to building a neural net this is also an easily differentiable operation.</li>
<li>Products are not as numerically stable as they tend to converge quickly to zero or infinity. Imagine you are multiplying many small decimals (which in our case definitely applies), the product will be extremely small.</li>
</ul>
<p>Essentially, taking the natural log of our probabilities gives us a nice smooth function that also squeezes extreme numbers to be within a smaller range while still maximizing our original objective.</p>
<p>In machine learning, when optimizing our model with a loss function, semantically lower is better. We can use the <strong>negative log-likelihood</strong> which still retains the same properties, but is now being minimized instead of maximized. Given our model’s output probabilities, we calculate <span class="math inline">\(-\log(\hat{y})\)</span> where <span class="math inline">\(\hat{y}\)</span> is the prediction for the true character class and then find the mean across every sample for an aggregate loss.</p>
<p><a href="#fig-nll-plot" class="quarto-xref">Figure&nbsp;2</a> shows the negative log function to visualize how higher probabilities lower the loss. We can interpret this as having higher confidence in the correct character leads to a lower loss while having lower confidence leads to a higher loss.</p>
<div id="cell-fig-nll-plot" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="op">-</span>torch.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">100</span>).log())</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>plt.xticks(</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>    ticks<span class="op">=</span>torch.arange(<span class="dv">0</span>, <span class="dv">110</span>, <span class="dv">10</span>), labels<span class="op">=</span>[<span class="ss">f"</span><span class="sc">{</span>i<span class="op">/</span><span class="dv">100</span><span class="sc">:.1f}</span><span class="ss">"</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">110</span>, <span class="dv">10</span>)]</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-nll-plot" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nll-plot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-nll-plot-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nll-plot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Plot of negative log function
</figcaption>
</figure>
</div>
</div>
</div>
<p>Calculating the negative log-likelihood across our dataset:</p>
<div id="cell-30" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>log_likelihood <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>n_ele <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n <span class="kw">in</span> names:</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>    chs <span class="op">=</span> [<span class="st">"."</span>] <span class="op">+</span> <span class="bu">list</span>(n) <span class="op">+</span> [<span class="st">"."</span>]</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> ch1, ch2 <span class="kw">in</span> <span class="bu">zip</span>(chs, chs[<span class="dv">1</span>:]):</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>        ix1, ix2 <span class="op">=</span> stoi[ch1], stoi[ch2]</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>        log_likelihood <span class="op">+=</span> torch.log(P[ix1, ix2])</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>        n_ele <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>nll <span class="op">=</span> <span class="op">-</span>log_likelihood</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>nll <span class="op">=</span> <span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>nll<span class="op">/</span>n_ele <span class="op">=</span> <span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>nll = tensor(2235483.2500)
nll/n_ele = tensor(2.5296)</code></pre>
</div>
</div>
<p>Great! We now have a metric to summarize our classifier to compare with other models. We might be interested in seeing the loss from our uniform, equal probability model. Let’s try that out now.</p>
<div id="cell-32" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>P_unif <span class="op">=</span> torch.ones_like(N, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="co"># P_unif /= P_unif.sum(dim=1, keepdim=True)</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>P_unif <span class="op">/=</span> vocab_size</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> torch.allclose(</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>    torch.ones(P_unif.shape[<span class="dv">0</span>], dtype<span class="op">=</span>torch.float32), P_unif.<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-33" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>log_likelihood <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>n_ele <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n <span class="kw">in</span> names:</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>    chs <span class="op">=</span> [<span class="st">"."</span>] <span class="op">+</span> <span class="bu">list</span>(n) <span class="op">+</span> [<span class="st">"."</span>]</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> ch1, ch2 <span class="kw">in</span> <span class="bu">zip</span>(chs, chs[<span class="dv">1</span>:]):</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>        ix1, ix2 <span class="op">=</span> stoi[ch1], stoi[ch2]</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>        log_likelihood <span class="op">+=</span> torch.log(P_unif[ix1, ix2])</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>        n_ele <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>nll <span class="op">=</span> <span class="op">-</span>log_likelihood</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>nll <span class="op">=</span> <span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>nll<span class="op">/</span>n_ele <span class="op">=</span> <span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>nll = tensor(2949547.5000)
nll/n_ele = tensor(3.3376)</code></pre>
</div>
</div>
<p>In terms of metrics alone, the bigram model does seem to have an edge over the uniform model. Qualitatively, the bigram model outputs also seem to be a bit better for creating the perfect warlock. However, there’s one little “bug” we should be on the look out for and that’s an infinite log loss.</p>
<p>Remember how our character frequencies had some 0 values for specific bigrams? If our model encounters these bigrams like <code>qa</code>, then the probability will be 0. Taking the natural log of 0 is undefined and this results in an infinite loss.</p>
<div id="cell-35" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>log_likelihood <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>n_ele <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n <span class="kw">in</span> [<span class="st">"qasa"</span>]:</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>    chs <span class="op">=</span> [<span class="st">"."</span>] <span class="op">+</span> <span class="bu">list</span>(n) <span class="op">+</span> [<span class="st">"."</span>]</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> ch1, ch2 <span class="kw">in</span> <span class="bu">zip</span>(chs, chs[<span class="dv">1</span>:]):</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>        ix1, ix2 <span class="op">=</span> stoi[ch1], stoi[ch2]</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>        log_likelihood <span class="op">+=</span> torch.log(P[ix1, ix2])</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>        n_ele <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>nll <span class="op">=</span> <span class="op">-</span>log_likelihood</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>nll <span class="op">=</span> <span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>nll<span class="op">/</span>n_ele <span class="op">=</span> <span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>nll = tensor(inf)
nll/n_ele = tensor(inf)</code></pre>
</div>
</div>
<p>To address this, we can add some smoothing by adding a small amount of fake counts to all the frequency values. The more counts we add, the more uniform our model becomes. However, the model will be able to parse these rare bigrams.</p>
<div id="cell-37" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>P <span class="op">=</span> (N <span class="op">+</span> <span class="dv">1</span>).<span class="bu">float</span>()</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>P <span class="op">/=</span> P.<span class="bu">sum</span>(</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>    dim<span class="op">=</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>)  <span class="co"># By keeping the row dimension, the row sum array will be broadcasted across the rest of P</span></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> torch.allclose(</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>    torch.ones(P.shape[<span class="dv">0</span>], dtype<span class="op">=</span>torch.float32), P.<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>)  <span class="co"># Check to see that every row sums to 1 in our probability matrix</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-38" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>log_likelihood <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>n_ele <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n <span class="kw">in</span> [<span class="st">"qasa"</span>]:</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>    chs <span class="op">=</span> [<span class="st">"."</span>] <span class="op">+</span> <span class="bu">list</span>(n) <span class="op">+</span> [<span class="st">"."</span>]</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> ch1, ch2 <span class="kw">in</span> <span class="bu">zip</span>(chs, chs[<span class="dv">1</span>:]):</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>        ix1, ix2 <span class="op">=</span> stoi[ch1], stoi[ch2]</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>        log_prob <span class="op">=</span> torch.log(P[ix1, ix2])</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>        log_likelihood <span class="op">+=</span> log_prob</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>        n_ele <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Bigram: </span><span class="sc">{</span>ch1 <span class="op">+</span> ch2<span class="sc">}</span><span class="ss">, Log-prob: </span><span class="sc">{</span>log_prob<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>nll <span class="op">=</span> <span class="op">-</span>log_likelihood</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>nll <span class="op">=</span> <span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>nll<span class="op">/</span>n_ele <span class="op">=</span> <span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Bigram: .q, Log-prob: -3.676
Bigram: qa, Log-prob: -7.838
Bigram: as, Log-prob: -3.587
Bigram: sa, Log-prob: -2.492
Bigram: a., Log-prob: -3.300
nll = tensor(20.8932)
nll/n_ele = tensor(4.1786)</code></pre>
</div>
</div>
<p><code>qa</code> still reflects a highly unlikely bigram compared to the others, but no longer outputs an infinite log loss. Alright, we have a decent roll of the dice, but we might speculate that a bigram model is too simple. One route we can take is to expand the <strong>context size</strong>—the length of tokens predicting the next token. We can try a <strong>trigram model</strong> instead.</p>
</section>
<section id="trigram-model" class="level1">
<h1>Trigram Model</h1>
<p>In the trigram model, a sequence of 2 characters will predict the third one. In <code>abcd</code>, <code>ab -&gt; c</code>. One way we can represent this is by increasing our frequency tensor’s dimension to 3. Each dimension will represent the occurrence of each character in a trigram.</p>
<p>Using a 3D tensor instead of a 2D tensor (<span class="math inline">\(29^2 \times 29\)</span> where each row represents a bigram predicting the next character) will also allow us to use the same encoder and decoder we used before.</p>
<div id="cell-42" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> torch.zeros((<span class="bu">len</span>(stoi), <span class="bu">len</span>(stoi), <span class="bu">len</span>(stoi)), dtype<span class="op">=</span>torch.int32)</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n <span class="kw">in</span> names:</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>    chs <span class="op">=</span> [<span class="st">"."</span>] <span class="op">+</span> <span class="bu">list</span>(n) <span class="op">+</span> [<span class="st">"."</span>]</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> ch1, ch2, ch3 <span class="kw">in</span> <span class="bu">zip</span>(chs, chs[<span class="dv">1</span>:], chs[<span class="dv">2</span>:]):</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>        idx1, idx2, idx3 <span class="op">=</span> stoi[ch1], stoi[ch2], stoi[ch3]</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>        N[idx1, idx2, idx3] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>N</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="34">
<pre><code>tensor([[[  0,   0,   0,  ...,   0,   0,   0],
         [  0,   0,   0,  ...,   0,   0,   0],
         [  0,   0,   0,  ...,   0,   0,   0],
         ...,
         [  0,   0,   0,  ...,   0,   0,   0],
         [  0,   0,   0,  ...,   0,   0,   0],
         [  0,   0,   0,  ...,   0,  47,   0]],

        [[  0,   0,   0,  ...,   0,   0,   0],
         [  0,   0,   0,  ...,   0,   0,   0],
         [  0,   0,   0,  ...,   0,   0,   0],
         ...,
         [  0,   0,   0,  ...,   0,   0,   0],
         [  0,   0,   0,  ...,   0,   0,   0],
         [  0,   0,   0,  ...,   0,   0,   0]],

        [[  0,   0,   0,  ...,   0,   0,   0],
         [  0,   0,   0,  ...,   0,   0,   0],
         [  0,   0,   0,  ...,   0,   0,   0],
         ...,
         [  0,   0,   0,  ...,   0,   0,   0],
         [  0,   0,   0,  ...,   0,   0,   0],
         [  0,   0,   0,  ...,   0,  71,   0]],

        ...,

        [[  0,   0,   0,  ...,   0,   0,   0],
         [  0,   0,   0,  ...,   0,   0,   5],
         [  0,   0,   0,  ...,   0,   0,   0],
         ...,
         [  0,   0,   0,  ...,   0,   0,   0],
         [  0,   0,   0,  ...,   0,   0,   0],
         [  0,   0,   0,  ...,   0,   5,   0]],

        [[  0,   0,   0,  ...,   0,   0,   0],
         [  0,   0,   0,  ...,   0,   0,   0],
         [  0,   0,   0,  ...,   0,   0,   0],
         ...,
         [  0,   0,   0,  ...,   0,  11,  19],
         [  0,   0,   0,  ...,   0,   0,   0],
         [  0,   0,   0,  ...,   0,  93,   0]],

        [[  0,   0,   0,  ...,   0,   0,   0],
         [  0,   0,   0,  ...,   0,   0,   7],
         [  0,   0,   0,  ...,   0,   0,   0],
         ...,
         [  0,   0,   0,  ...,   0,   0,   0],
         [  0,   0,   0,  ...,   0,   0,   0],
         [  0, 288,   0,  ...,   0,  95,  17]]], dtype=torch.int32)</code></pre>
</div>
</div>
<section id="generating-characters-for-the-trigram-model" class="level2">
<h2 class="anchored" data-anchor-id="generating-characters-for-the-trigram-model">Generating Characters for the Trigram Model</h2>
<p>Now we can generate some characters using the same method from the bigram model. I’m curious what names we’ll get if we start with the first letter of my own name.</p>
<div id="cell-45" class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>P <span class="op">=</span> (N <span class="op">+</span> <span class="dv">1</span>).<span class="bu">float</span>()  <span class="co"># Model smoothing</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>P <span class="op">/=</span> P.<span class="bu">sum</span>(</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>    dim<span class="op">=-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>)  <span class="co"># Normalize the counts along the last dimension to be a probability distribution</span></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">1234</span>)  <span class="co"># Reproducibility</span></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">15</span>):</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> [<span class="st">"y"</span>]</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>    ix1, ix2 <span class="op">=</span> stoi[<span class="st">"."</span>], stoi[<span class="st">"y"</span>]  <span class="co"># Start with ".y"</span></span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>        p <span class="op">=</span> P[ix1, ix2]  <span class="co"># Get the probability for current bigram</span></span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a>        ix1 <span class="op">=</span> ix2  <span class="co"># Shift the "context window" by 1</span></span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a>        ix2 <span class="op">=</span> torch.multinomial(</span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a>            p, num_samples<span class="op">=</span><span class="dv">1</span>, replacement<span class="op">=</span><span class="va">True</span>, generator<span class="op">=</span>g</span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a>        ).item()  <span class="co"># Get the next predicted character</span></span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> ix2 <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a>        out.append(itos[ix2])</span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">""</span>.join(out))  <span class="co"># Print the final result</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>yaundros alodeafeinimmelddarn khalgovanvar chadrizza
yurzena
yauluan dhund
yaamlespmaerhrammyrrath
yaunzenelvid dus ket
yaufein
yaudaen kryl
yazzath
yatanlyrin tel
yaan drae
yaurae
yuillanorzzavaddtlimas zak al
yaufeindaendalauvryle helani
yazza
yund</code></pre>
</div>
</div>
<p>And now when we evaluate the trigram model loss:</p>
<div id="cell-47" class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>log_likelihood <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>n_ele <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n <span class="kw">in</span> names:</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>    chs <span class="op">=</span> [<span class="st">"."</span>, <span class="op">*</span>n, <span class="st">"."</span>]</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> ch1, ch2, ch3 <span class="kw">in</span> <span class="bu">zip</span>(chs, chs[<span class="dv">1</span>:], chs[<span class="dv">2</span>:]):</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>        ix1, ix2, ix3 <span class="op">=</span> stoi[ch1], stoi[ch2], stoi[ch3]</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>        log_likelihood <span class="op">+=</span> torch.log(P[ix1, ix2, ix3])</span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>        n_ele <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a>nll <span class="op">=</span> <span class="op">-</span>log_likelihood</span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>nll <span class="op">=</span> <span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>nll<span class="op">/</span>n_ele <span class="op">=</span> <span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>nll = tensor(1523841.2500)
nll/n_ele = tensor(1.8337)</code></pre>
</div>
</div>
<p>The trigram model seems to perform better than the bigram model on the full data and we love ourselves a goblin-sounding <code>yazza</code>. Overall, n-gram models are straightforward and they do a good job of optimizing our loss function in this setting.</p>
<p>However, a more flexible framework is to use gradient-based optimization in a neural net. An overview: Instead of explicitly counting characters, we can tune weights with gradient descent that, multiplied by our inputs, will create logits. These predicted logits will then be transformed into a probability distribution via <strong>softmax</strong> which can be used to predict the next token. As Karpathy mentions, the simple neural net we are creating will ultimately get us to a similar place as the counting approach, but we will be able to expand on the architecture much further than the counting models in later parts.</p>
</section>
</section>
<section id="neural-net---collectors-edition" class="level1">
<h1>Neural Net - Collector’s Edition</h1>
<p>We can recreate both the bigram and trigram models easily under this framework. The first step is to load the data. Instead of explicitly counting the frequency of each character / pair, we will store the an array of encoded inputs into <code>X</code> and target characters into <code>y</code>.</p>
<section id="bigram-model" class="level2">
<h2 class="anchored" data-anchor-id="bigram-model">Bigram Model</h2>
<div id="cell-51" class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> [], []</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n <span class="kw">in</span> names:</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>    chs <span class="op">=</span> [<span class="st">"."</span>, <span class="op">*</span>n, <span class="st">"."</span>]  <span class="co"># Pad each name with the terminating chars</span></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> ch1, ch2 <span class="kw">in</span> <span class="bu">zip</span>(chs, chs[<span class="dv">1</span>:]):</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>        ix1, ix2 <span class="op">=</span> stoi[ch1], stoi[ch2]</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>        X.append(ix1)</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>        y.append(ix2)</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> torch.tensor(X)</span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> torch.tensor(y)</span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>num_X <span class="op">=</span> X.nelement()</span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Number of examples:"</span>, num_X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Number of examples: 883744</code></pre>
</div>
</div>
<div id="cell-52" class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>X, y</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="46">
<pre><code>(tensor([ 0, 13, 20,  ...,  2, 22, 10]),
 tensor([13, 20,  7,  ..., 22, 10,  0]))</code></pre>
</div>
</div>
<p>The second step is to feed our character indices into the model. However, we don’t want to directly feed in the integer values. For each index, we’d like to select the corresponding row in the weights matrix through matrix multiplication.</p>
<p>This is where <strong>one-hot encoding</strong> comes in! Each integer index <span class="math inline">\(i\)</span> can be represented as a vector that is all 0s except for the <span class="math inline">\(i\)</span> position.</p>
<p><span class="math display">\[
\left[
    \begin{matrix}
    1 &amp; 2 &amp; 3 &amp; 4 \\
    \end{matrix}
\right]
\longrightarrow
\left[
    \begin{matrix}
    1 &amp; 0 &amp; 0 &amp; 0 \\
    0 &amp; 1 &amp; 0 &amp; 0 \\
    0 &amp; 0 &amp; 1 &amp; 0 \\
    0 &amp; 0 &amp; 0 &amp; 1
    \end{matrix}
\right]
\]</span></p>
<p>When multiplied with our randomly initialized weight matrix <span class="math inline">\(W\)</span>, the one-hot encoded vectors will select the corresponding rows in <span class="math inline">\(W\)</span> similar to how we selected the rows from our probability matrix during character generation.</p>
<div id="cell-54" class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">1234</span>)</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>x_enc <span class="op">=</span> F.one_hot(X, num_classes<span class="op">=</span><span class="bu">len</span>(stoi)).<span class="bu">float</span>()  <span class="co"># One-hot encode our input indices</span></span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> torch.randn(</span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a>    (<span class="bu">len</span>(stoi), <span class="bu">len</span>(stoi)), dtype<span class="op">=</span>torch.float32, generator<span class="op">=</span>g</span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>)  <span class="co"># Initialize our weight matrix with random Normal values, shape (29, 29)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-55" class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Demonstrating the row selection with one-hot vectors</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>first_row <span class="op">=</span> F.one_hot(torch.tensor(<span class="dv">0</span>), num_classes<span class="op">=</span><span class="bu">len</span>(stoi)).<span class="bu">float</span>()</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Test one-hot vector selects first row?"</span>, torch.<span class="bu">all</span>(W[<span class="dv">0</span>] <span class="op">==</span> (first_row <span class="op">@</span> W)).item()</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Test one-hot vector selects first row? True</code></pre>
</div>
</div>
<div id="cell-56" class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> x_enc <span class="op">@</span> W  <span class="co"># Linear layer outputs log-counts / logits for all observations in X</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>logits</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="49">
<pre><code>tensor([[-0.1117, -0.4966,  0.1631,  ...,  0.2207,  0.2463, -1.3248],
        [-1.4317,  2.1155, -1.1853,  ...,  1.4487, -1.5155, -1.2364],
        [ 1.4007, -1.0216, -0.5113,  ...,  0.0163,  0.0612, -2.6981],
        ...,
        [ 0.6808,  0.7244,  0.0323,  ...,  0.4308,  0.2862, -0.2481],
        [ 1.2134,  0.8028, -0.4904,  ..., -0.2103, -0.9933, -0.2486],
        [ 0.6091,  0.6668, -1.1206,  ..., -0.9728, -1.2452, -0.0115]])</code></pre>
</div>
</div>
<p>By multiplying our one-hot encoded inputs with our weights, we produce log-counts / logits. Logits are raw, unnormalized scores that the model outputs typically before applying some activation function.</p>
<p>Taking a quick glance at our logits, these can’t be probabilities! We have a bunch of positive and negative numbers, some of which are greater than 1. To represent probabilities, the sum across the columns should be 1.</p>
<p>We’d like to perform an operation similar to the <strong>sigmoid function</strong> which transforms a real number to be between 0 and 1, but this time the sum across <strong>a vector of values</strong> should be 1. This is exactly what softmax is: a generalization of sigmoid function to multiple dimensions!</p>
<p>We normalize our logits into a probability distribution using softmax. All softmax does is it divides an exponentiated input vector by the sum of all its exponentiated values.</p>
<p><span class="math display">\[
\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}}
\]</span></p>
<p>The sum in the denominator mirrors what we did with the character counts previously (integers, greater than or equal to 0). There we just divided every count by the sum of the counts across the columns.</p>
<p>If you think about it, softmax gives us a good representation of what we’re trying to do in multiclass classification where we’d like to utilize a probability space across all the classes after we train our model.</p>
<div id="cell-58" class="cell" data-execution_count="50">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>counts <span class="op">=</span> logits.exp()  <span class="co"># Exponentiate the logits</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> counts <span class="op">/</span> counts.<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> torch.allclose(torch.ones(probs.shape[<span class="dv">0</span>], dtype<span class="op">=</span>torch.float32), probs.<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>logits[<span class="dv">0</span>], probs[<span class="dv">0</span>]  <span class="co"># Just to compare some values</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="50">
<pre><code>(tensor([-0.1117, -0.4966,  0.1631, -0.8817,  0.0539,  0.6684, -0.0597, -0.4675,
         -0.2153,  0.8840, -0.7584, -0.3689, -0.3424, -1.4020,  0.3206, -1.0219,
          0.7988, -0.0923, -0.7049, -1.6024,  0.2891,  0.4899, -0.3853, -0.7120,
         -0.1706, -1.4594,  0.2207,  0.2463, -1.3248]),
 tensor([0.0336, 0.0229, 0.0443, 0.0156, 0.0397, 0.0734, 0.0354, 0.0236, 0.0303,
         0.0910, 0.0176, 0.0260, 0.0267, 0.0093, 0.0518, 0.0135, 0.0836, 0.0343,
         0.0186, 0.0076, 0.0502, 0.0614, 0.0256, 0.0185, 0.0317, 0.0087, 0.0469,
         0.0481, 0.0100]))</code></pre>
</div>
</div>
</section>
<section id="optimizing-the-neural-net" class="level2">
<h2 class="anchored" data-anchor-id="optimizing-the-neural-net">Optimizing the Neural Net</h2>
<p>With probabilities in hand, we can now calculate the loss and create a training loop with backpropagation to update the weights.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>We won’t be going over backprop here, but you can read all about it in this Stanford CS231n resource. Also, you can look at a simple vectorized feedforward net I implemented through GitHub.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="Ensuring Autodiff">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Ensuring Autodiff
</div>
</div>
<div class="callout-body-container callout-body">
<p>In order for PyTorch to automatically record and calculate gradients, we’ll need to specify <code>requires_grad=True</code> in the attributes of tensors involved in the model.</p>
</div>
</div>
<section id="training-loop" class="level3">
<h3 class="anchored" data-anchor-id="training-loop">Training Loop</h3>
<div id="cell-62" class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">1234</span>)</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> torch.randn((<span class="bu">len</span>(stoi), <span class="bu">len</span>(stoi)), generator<span class="op">=</span>g, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Forward pass</span></span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a>    x_enc <span class="op">=</span> F.one_hot(X, num_classes<span class="op">=</span><span class="bu">len</span>(stoi)).<span class="bu">float</span>()</span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> x_enc <span class="op">@</span> W</span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a>    counts <span class="op">=</span> logits.exp()</span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a>    probs <span class="op">=</span> counts <span class="op">/</span> counts.<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-12"><a href="#cb51-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate negative log-likelihood</span></span>
<span id="cb51-13"><a href="#cb51-13" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> <span class="op">-</span>probs[torch.arange(num_X), y].log().mean()</span>
<span id="cb51-14"><a href="#cb51-14" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">, Loss: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb51-15"><a href="#cb51-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-16"><a href="#cb51-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Backward pass</span></span>
<span id="cb51-17"><a href="#cb51-17" aria-hidden="true" tabindex="-1"></a>    W.grad <span class="op">=</span> <span class="va">None</span>  <span class="co"># Zero out the gradient before backprop</span></span>
<span id="cb51-18"><a href="#cb51-18" aria-hidden="true" tabindex="-1"></a>    loss.backward()  <span class="co"># Backprop</span></span>
<span id="cb51-19"><a href="#cb51-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-20"><a href="#cb51-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update step</span></span>
<span id="cb51-21"><a href="#cb51-21" aria-hidden="true" tabindex="-1"></a>    lr <span class="op">=</span> <span class="dv">50</span>  <span class="co"># Can use a very high learning rate on our simple data</span></span>
<span id="cb51-22"><a href="#cb51-22" aria-hidden="true" tabindex="-1"></a>    W.data <span class="op">+=</span> <span class="op">-</span>lr <span class="op">*</span> W.grad</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 0, Loss: 3.8228
Epoch 1, Loss: 3.5151
Epoch 2, Loss: 3.3178
Epoch 3, Loss: 3.1808
Epoch 4, Loss: 3.0822
Epoch 5, Loss: 3.0082
Epoch 6, Loss: 2.9509
Epoch 7, Loss: 2.9054
Epoch 8, Loss: 2.8684
Epoch 9, Loss: 2.8378
Epoch 10, Loss: 2.8120
Epoch 11, Loss: 2.7899
Epoch 12, Loss: 2.7708
Epoch 13, Loss: 2.7541
Epoch 14, Loss: 2.7395
Epoch 15, Loss: 2.7265
Epoch 16, Loss: 2.7149
Epoch 17, Loss: 2.7045
Epoch 18, Loss: 2.6951
Epoch 19, Loss: 2.6867
Epoch 20, Loss: 2.6790
Epoch 21, Loss: 2.6719
Epoch 22, Loss: 2.6655
Epoch 23, Loss: 2.6596
Epoch 24, Loss: 2.6542
Epoch 25, Loss: 2.6491
Epoch 26, Loss: 2.6445
Epoch 27, Loss: 2.6402
Epoch 28, Loss: 2.6362
Epoch 29, Loss: 2.6324
Epoch 30, Loss: 2.6290
Epoch 31, Loss: 2.6257
Epoch 32, Loss: 2.6226
Epoch 33, Loss: 2.6198
Epoch 34, Loss: 2.6171
Epoch 35, Loss: 2.6145
Epoch 36, Loss: 2.6121
Epoch 37, Loss: 2.6098
Epoch 38, Loss: 2.6077
Epoch 39, Loss: 2.6056
Epoch 40, Loss: 2.6037
Epoch 41, Loss: 2.6019
Epoch 42, Loss: 2.6001
Epoch 43, Loss: 2.5984
Epoch 44, Loss: 2.5968
Epoch 45, Loss: 2.5953
Epoch 46, Loss: 2.5938
Epoch 47, Loss: 2.5924
Epoch 48, Loss: 2.5910
Epoch 49, Loss: 2.5897
Epoch 50, Loss: 2.5885
Epoch 51, Loss: 2.5873
Epoch 52, Loss: 2.5861
Epoch 53, Loss: 2.5850
Epoch 54, Loss: 2.5839
Epoch 55, Loss: 2.5829
Epoch 56, Loss: 2.5819
Epoch 57, Loss: 2.5809
Epoch 58, Loss: 2.5800
Epoch 59, Loss: 2.5791
Epoch 60, Loss: 2.5782
Epoch 61, Loss: 2.5773
Epoch 62, Loss: 2.5765
Epoch 63, Loss: 2.5757
Epoch 64, Loss: 2.5750
Epoch 65, Loss: 2.5742
Epoch 66, Loss: 2.5735
Epoch 67, Loss: 2.5728
Epoch 68, Loss: 2.5721
Epoch 69, Loss: 2.5714
Epoch 70, Loss: 2.5708
Epoch 71, Loss: 2.5701
Epoch 72, Loss: 2.5695
Epoch 73, Loss: 2.5689
Epoch 74, Loss: 2.5684
Epoch 75, Loss: 2.5678
Epoch 76, Loss: 2.5672
Epoch 77, Loss: 2.5667
Epoch 78, Loss: 2.5662
Epoch 79, Loss: 2.5657
Epoch 80, Loss: 2.5652
Epoch 81, Loss: 2.5647
Epoch 82, Loss: 2.5642
Epoch 83, Loss: 2.5638
Epoch 84, Loss: 2.5633
Epoch 85, Loss: 2.5629
Epoch 86, Loss: 2.5624
Epoch 87, Loss: 2.5620
Epoch 88, Loss: 2.5616
Epoch 89, Loss: 2.5612
Epoch 90, Loss: 2.5608
Epoch 91, Loss: 2.5604
Epoch 92, Loss: 2.5601
Epoch 93, Loss: 2.5597
Epoch 94, Loss: 2.5594
Epoch 95, Loss: 2.5590
Epoch 96, Loss: 2.5587
Epoch 97, Loss: 2.5583
Epoch 98, Loss: 2.5580
Epoch 99, Loss: 2.5577
Epoch 100, Loss: 2.5574
Epoch 101, Loss: 2.5571
Epoch 102, Loss: 2.5568
Epoch 103, Loss: 2.5565
Epoch 104, Loss: 2.5562
Epoch 105, Loss: 2.5559
Epoch 106, Loss: 2.5556
Epoch 107, Loss: 2.5553
Epoch 108, Loss: 2.5551
Epoch 109, Loss: 2.5548
Epoch 110, Loss: 2.5546
Epoch 111, Loss: 2.5543
Epoch 112, Loss: 2.5541
Epoch 113, Loss: 2.5538
Epoch 114, Loss: 2.5536
Epoch 115, Loss: 2.5534
Epoch 116, Loss: 2.5531
Epoch 117, Loss: 2.5529
Epoch 118, Loss: 2.5527
Epoch 119, Loss: 2.5525
Epoch 120, Loss: 2.5523
Epoch 121, Loss: 2.5520
Epoch 122, Loss: 2.5518
Epoch 123, Loss: 2.5516
Epoch 124, Loss: 2.5514
Epoch 125, Loss: 2.5512
Epoch 126, Loss: 2.5511
Epoch 127, Loss: 2.5509
Epoch 128, Loss: 2.5507
Epoch 129, Loss: 2.5505
Epoch 130, Loss: 2.5503
Epoch 131, Loss: 2.5501
Epoch 132, Loss: 2.5500
Epoch 133, Loss: 2.5498
Epoch 134, Loss: 2.5496
Epoch 135, Loss: 2.5495
Epoch 136, Loss: 2.5493
Epoch 137, Loss: 2.5492
Epoch 138, Loss: 2.5490
Epoch 139, Loss: 2.5488
Epoch 140, Loss: 2.5487
Epoch 141, Loss: 2.5485
Epoch 142, Loss: 2.5484
Epoch 143, Loss: 2.5482
Epoch 144, Loss: 2.5481
Epoch 145, Loss: 2.5480
Epoch 146, Loss: 2.5478
Epoch 147, Loss: 2.5477
Epoch 148, Loss: 2.5476
Epoch 149, Loss: 2.5474
Epoch 150, Loss: 2.5473
Epoch 151, Loss: 2.5472
Epoch 152, Loss: 2.5470
Epoch 153, Loss: 2.5469
Epoch 154, Loss: 2.5468
Epoch 155, Loss: 2.5467
Epoch 156, Loss: 2.5465
Epoch 157, Loss: 2.5464
Epoch 158, Loss: 2.5463
Epoch 159, Loss: 2.5462
Epoch 160, Loss: 2.5461
Epoch 161, Loss: 2.5460
Epoch 162, Loss: 2.5459
Epoch 163, Loss: 2.5457
Epoch 164, Loss: 2.5456
Epoch 165, Loss: 2.5455
Epoch 166, Loss: 2.5454
Epoch 167, Loss: 2.5453
Epoch 168, Loss: 2.5452
Epoch 169, Loss: 2.5451
Epoch 170, Loss: 2.5450
Epoch 171, Loss: 2.5449
Epoch 172, Loss: 2.5448
Epoch 173, Loss: 2.5447
Epoch 174, Loss: 2.5446
Epoch 175, Loss: 2.5445
Epoch 176, Loss: 2.5445
Epoch 177, Loss: 2.5444
Epoch 178, Loss: 2.5443
Epoch 179, Loss: 2.5442
Epoch 180, Loss: 2.5441
Epoch 181, Loss: 2.5440
Epoch 182, Loss: 2.5439
Epoch 183, Loss: 2.5438
Epoch 184, Loss: 2.5438
Epoch 185, Loss: 2.5437
Epoch 186, Loss: 2.5436
Epoch 187, Loss: 2.5435
Epoch 188, Loss: 2.5434
Epoch 189, Loss: 2.5433
Epoch 190, Loss: 2.5433
Epoch 191, Loss: 2.5432
Epoch 192, Loss: 2.5431
Epoch 193, Loss: 2.5430
Epoch 194, Loss: 2.5430
Epoch 195, Loss: 2.5429
Epoch 196, Loss: 2.5428
Epoch 197, Loss: 2.5427
Epoch 198, Loss: 2.5427
Epoch 199, Loss: 2.5426</code></pre>
</div>
</div>
<p>The expected loss should converge to around the same point that our counting model did. Why would we want to use the neural net framework?</p>
<p>A couple of points to think about from the Karpathy’s lecture:</p>
<ul>
<li>As the number of tokens we use to predict the next token increases, the counting approach doesn’t scale. For an n-gram model, the frequency matrix will either have shape <span class="math inline">\(29^n \times 29\)</span> for a 2D tensor or increase the number of dimensions to an ND tensor. You can imagine this would get unyieldy quickly.</li>
<li>Although the neural net took on a similar approach to counting, there are many more settings we can improve on with the neural net. As we iterate on the architecture, the way we output the logits (the forward pass) will change but everything else will remain the same.</li>
</ul>
</section>
</section>
</section>
<section id="trigram-neural-net-optimizing-code" class="level1">
<h1>Trigram Neural Net, Optimizing Code</h1>
<p>We’ll implement the trigram neural net in a similar way to how we made the bigram net. Our input <code>X</code> will now be 2D and take in arrays of encoded indices.</p>
<div id="cell-66" class="cell" data-execution_count="62">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>X_tri, y_tri <span class="op">=</span> [], []</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n <span class="kw">in</span> names:</span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>    chs <span class="op">=</span> [<span class="st">"."</span>] <span class="op">+</span> <span class="bu">list</span>(n) <span class="op">+</span> [<span class="st">"."</span>]</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> ch1, ch2, ch3 <span class="kw">in</span> <span class="bu">zip</span>(chs, chs[<span class="dv">1</span>:], chs[<span class="dv">2</span>:]):</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>        ix1, ix2, ix3 <span class="op">=</span> stoi[ch1], stoi[ch2], stoi[ch3]</span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>        X_tri.append([ix1, ix2])</span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>        y_tri.append(ix3)</span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a>X_tri, y_tri <span class="op">=</span> torch.tensor(X_tri), torch.tensor(y_tri)</span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a>num_X <span class="op">=</span> X_tri.shape[<span class="dv">0</span>]</span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Number of examples:"</span>, num_X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Number of examples: 831018</code></pre>
</div>
</div>
<p>Before we train, we can condense our code and make it more efficient using PyTorch mechanisms.</p>
<section id="one-hot-encoding-seems-wasteful" class="level2">
<h2 class="anchored" data-anchor-id="one-hot-encoding-seems-wasteful">One-Hot Encoding Seems Wasteful…</h2>
<p>In the bigram neural net, all our one-hot encoding matrix is doing is indexing the <span class="math inline">\(i^{th}\)</span> row of the weight matrix when we multiply them together. We can eliminate the matrix multiplication entirely and use the indices themselves.</p>
<div id="cell-69" class="cell" data-execution_count="500">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> torch.<span class="bu">all</span>((x_enc <span class="op">@</span> W)[<span class="dv">0</span>] <span class="op">==</span> W[<span class="dv">0</span>]).item()</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>(x_enc <span class="op">@</span> W)[<span class="dv">0</span>], W[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="500">
<pre><code>(tensor([-3.0790, -3.1019, -3.0673, -0.0794,  0.6222,  0.0568,  0.7115, -0.1114,
         -1.2216, -0.0568, -0.0403,  0.5561,  0.2973,  0.6091, -0.0517,  0.3182,
          0.7183, -0.3287, -0.7147, -0.2213,  0.8389,  0.8457,  0.8208, -0.7500,
          0.5668, -0.2676, -1.3057, -0.7654, -0.2419],
        grad_fn=&lt;SelectBackward0&gt;),
 tensor([-3.0790, -3.1019, -3.0673, -0.0794,  0.6222,  0.0568,  0.7115, -0.1114,
         -1.2216, -0.0568, -0.0403,  0.5561,  0.2973,  0.6091, -0.0517,  0.3182,
          0.7183, -0.3287, -0.7147, -0.2213,  0.8389,  0.8457,  0.8208, -0.7500,
          0.5668, -0.2676, -1.3057, -0.7654, -0.2419],
        grad_fn=&lt;SelectBackward0&gt;))</code></pre>
</div>
</div>
<p>For the trigram model, our one-hot encoding would have looked a little different. We would have had two <span class="math inline">\(1 \times 29\)</span> arrays after one-hot encoding a single observation. To represent the bigram properly, we could flatten the input by reshaping it to be <span class="math inline">\(1 \times (29 \times 2)\)</span>.</p>
<div id="cell-71" class="cell">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>x_enc <span class="op">=</span> F.one_hot(X_tri, num_classes<span class="op">=</span><span class="bu">len</span>(stoi))</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>x_enc.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="bu">len</span>(stoi) <span class="op">*</span> <span class="dv">2</span>)[</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>    <span class="dv">0</span></span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>]  <span class="co"># Example of the first one-hot encoded input for trigram</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0])</code></pre>
</div>
</div>
<p>However, to directly index into the weight matrix we’ll instead add the corresponding weights of the two input one-hot vectors.</p>
<div id="cell-73" class="cell" data-execution_count="68">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">1234</span>)</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>W_tri <span class="op">=</span> torch.randn(<span class="bu">len</span>(stoi) <span class="op">*</span> <span class="dv">2</span>, <span class="bu">len</span>(stoi), generator<span class="op">=</span>g, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a>    W_tri[X_tri[:, <span class="dv">0</span>]] <span class="op">+</span> W_tri[X_tri[:, <span class="dv">1</span>] <span class="op">+</span> <span class="bu">len</span>(stoi)]</span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a>)  <span class="co"># If we were to work out the math this should be the same as the one-hot matrix multiplication</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="68">
<pre><code>tensor([[-1.3989,  0.3646, -0.5769,  ...,  1.1644,  0.2497, -1.0169],
        [-2.4248,  2.7207, -0.9457,  ...,  1.2563, -1.0174, -2.0139],
        [ 1.2271, -1.3894, -0.8112,  ..., -0.5733,  0.3006, -5.3611],
        ...,
        [-0.4182, -0.4081, -0.4229,  ...,  3.3730,  0.9064,  0.9942],
        [ 1.4163,  2.6578,  2.3376,  ...,  1.6067,  0.1897,  0.2689],
        [ 1.2769,  0.4757, -1.4108,  ...,  0.9325, -1.8790,  0.2165]],
       grad_fn=&lt;AddBackward0&gt;)</code></pre>
</div>
</div>
</section>
<section id="cross-entropy-loss" class="level2">
<h2 class="anchored" data-anchor-id="cross-entropy-loss">Cross Entropy Loss</h2>
<p>PyTorch has a function <code>F.cross_entropy()</code> which calculates the negative log-likelihood given the logits and target tensors.</p>
<p>From the docs:</p>
<blockquote class="blockquote">
<p>Compute the cross entropy loss between input logits and target.</p>
<p><strong>input</strong> (<em>Tensor</em>): Predicted <strong>unnormalized</strong> logits; see Shape section below for supported shapes</p>
<p><strong>target</strong> (<em>Tensor</em>): Ground truth class indices or class probabilities; see Shape section below for supported shapes.</p>
</blockquote>
<p>Essentially, this function expects our raw logits since it will softmax them for us and calculate the negative log-likelihood in a much more computationally efficient manner (leather boots -&gt; boots of swiftness upgrade).</p>
<p>This along with the will replace the bulk of our forward pass.</p>
</section>
<section id="trigram-model-1" class="level2">
<h2 class="anchored" data-anchor-id="trigram-model-1">Trigram Model</h2>
<div id="cell-76" class="cell" data-execution_count="94">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">1234</span>)</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>W_tri <span class="op">=</span> torch.randn(<span class="bu">len</span>(stoi) <span class="op">*</span> <span class="dv">2</span>, <span class="bu">len</span>(stoi), generator<span class="op">=</span>g, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Forward pass (prev)</span></span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># x_enc = F.one_hot(X_tri, num_classes=len(stoi)).float()</span></span>
<span id="cb61-9"><a href="#cb61-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># logits = x_enc @ W_tri</span></span>
<span id="cb61-10"><a href="#cb61-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># counts = logits.exp()</span></span>
<span id="cb61-11"><a href="#cb61-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># probs = counts / counts.sum(dim=1, keepdim=True)</span></span>
<span id="cb61-12"><a href="#cb61-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-13"><a href="#cb61-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Forward pass (new)</span></span>
<span id="cb61-14"><a href="#cb61-14" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> W_tri[X_tri[:, <span class="dv">0</span>]] <span class="op">+</span> W_tri[X_tri[:, <span class="dv">1</span>] <span class="op">+</span> <span class="bu">len</span>(stoi)]</span>
<span id="cb61-15"><a href="#cb61-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-16"><a href="#cb61-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate negative log-likelihood (prev)</span></span>
<span id="cb61-17"><a href="#cb61-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># loss = -probs[torch.arange(num_X), y_tri].log().mean()</span></span>
<span id="cb61-18"><a href="#cb61-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-19"><a href="#cb61-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate negative log-likelihood (new)</span></span>
<span id="cb61-20"><a href="#cb61-20" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> F.cross_entropy(logits, y_tri)</span>
<span id="cb61-21"><a href="#cb61-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-22"><a href="#cb61-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Print metrics every 10 epochs</span></span>
<span id="cb61-23"><a href="#cb61-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb61-24"><a href="#cb61-24" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">, Loss: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb61-25"><a href="#cb61-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-26"><a href="#cb61-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Backward pass</span></span>
<span id="cb61-27"><a href="#cb61-27" aria-hidden="true" tabindex="-1"></a>    W_tri.grad <span class="op">=</span> <span class="va">None</span></span>
<span id="cb61-28"><a href="#cb61-28" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb61-29"><a href="#cb61-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-30"><a href="#cb61-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update step</span></span>
<span id="cb61-31"><a href="#cb61-31" aria-hidden="true" tabindex="-1"></a>    lr <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb61-32"><a href="#cb61-32" aria-hidden="true" tabindex="-1"></a>    W_tri.data <span class="op">+=</span> <span class="op">-</span>lr <span class="op">*</span> W_tri.grad</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 0, Loss: 4.2444
Epoch 10, Loss: 2.6864
Epoch 20, Loss: 2.4972
Epoch 30, Loss: 2.4244
Epoch 40, Loss: 2.3866
Epoch 50, Loss: 2.3638
Epoch 60, Loss: 2.3487
Epoch 70, Loss: 2.3382
Epoch 80, Loss: 2.3303
Epoch 90, Loss: 2.3243
Epoch 100, Loss: 2.3196
Epoch 110, Loss: 2.3158
Epoch 120, Loss: 2.3126
Epoch 130, Loss: 2.3100
Epoch 140, Loss: 2.3077
Epoch 150, Loss: 2.3058
Epoch 160, Loss: 2.3041
Epoch 170, Loss: 2.3026
Epoch 180, Loss: 2.3013
Epoch 190, Loss: 2.3001</code></pre>
</div>
</div>
</section>
<section id="generating-characters-from-neural-net" class="level2">
<h2 class="anchored" data-anchor-id="generating-characters-from-neural-net">Generating Characters from Neural Net</h2>
<p>Finally, let’s see how we can generate characters from the neural net. A new band of adventurers await…</p>
<div id="cell-78" class="cell" data-execution_count="95">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">1234</span>)</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> [<span class="st">"y"</span>]</span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>    ix1, ix2 <span class="op">=</span> <span class="dv">0</span>, <span class="dv">27</span></span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> W_tri[[ix1]] <span class="op">+</span> W_tri[[ix2 <span class="op">+</span> <span class="bu">len</span>(stoi)]]</span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a>        counts <span class="op">=</span> logits.exp()</span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a>        p <span class="op">=</span> counts <span class="op">/</span> counts.<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb63-10"><a href="#cb63-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-11"><a href="#cb63-11" aria-hidden="true" tabindex="-1"></a>        ix1 <span class="op">=</span> ix2</span>
<span id="cb63-12"><a href="#cb63-12" aria-hidden="true" tabindex="-1"></a>        ix2 <span class="op">=</span> torch.multinomial(p, num_samples<span class="op">=</span><span class="dv">1</span>, replacement<span class="op">=</span><span class="va">True</span>, generator<span class="op">=</span>g).item()</span>
<span id="cb63-13"><a href="#cb63-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> ix2 <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb63-14"><a href="#cb63-14" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb63-15"><a href="#cb63-15" aria-hidden="true" tabindex="-1"></a>        out.append(itos[ix2])</span>
<span id="cb63-16"><a href="#cb63-16" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">""</span>.join(out))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>ylunorlelatorele feizminil'rar ho'gorlavzquvh drizzan
yr dorhimluan maelolaar
ym
ymaanichaunduvrth
yl
ym
ymwelvir dushont
ylym cl
yr
ymden kryl</code></pre>
</div>
</div>
<p>The generated names are a bit different than the counting model names. Our loss was also worse, but we’ll be undergoing a big architecture change soon which can capture the character relationships better. Note that we didn’t use a train/val/test split so our experiments could be overfitting, underfitting–it’s hard to tell. This post is just a demonstration of how we can model our names. We’ll create our splits next time.</p>
<p>With that, we’ll wrap up this session of our glorious campaign. It’s a long one, but we covered plenty of valuable information for our character creation needs. Stay tuned for Part 2!</p>
</section>
</section>
<section id="appendix" class="level1">
<h1>Appendix</h1>
<p>If you want to try the 2D tensor version of the trigram model, you can setup the data with a new encoder that maps all the possible character duos to integers.</p>
<div id="cell-82" class="cell" data-execution_count="58">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> itertools <span class="im">import</span> product</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> string</span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>full_vocab <span class="op">=</span> [<span class="st">"."</span>, <span class="st">"'"</span>, <span class="st">" "</span>] <span class="op">+</span> <span class="bu">list</span>(string.ascii_lowercase)</span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a>duos <span class="op">=</span> <span class="bu">sorted</span>([ch1 <span class="op">+</span> ch2 <span class="cf">for</span> ch1, ch2 <span class="kw">in</span> product(full_vocab, repeat<span class="op">=</span><span class="dv">2</span>)])</span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a>stoi_duos <span class="op">=</span> {bi: i <span class="cf">for</span> i, bi <span class="kw">in</span> <span class="bu">enumerate</span>(duos)}</span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a>itos_duos <span class="op">=</span> {i: bi <span class="cf">for</span> bi, i <span class="kw">in</span> stoi_duos.items()}</span>
<span id="cb65-8"><a href="#cb65-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-9"><a href="#cb65-9" aria-hidden="true" tabindex="-1"></a>stoi_duos</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="58">
<pre><code>{'  ': 0,
 " '": 1,
 ' .': 2,
 ' a': 3,
 ' b': 4,
 ' c': 5,
 ' d': 6,
 ' e': 7,
 ' f': 8,
 ' g': 9,
 ' h': 10,
 ' i': 11,
 ' j': 12,
 ' k': 13,
 ' l': 14,
 ' m': 15,
 ' n': 16,
 ' o': 17,
 ' p': 18,
 ' q': 19,
 ' r': 20,
 ' s': 21,
 ' t': 22,
 ' u': 23,
 ' v': 24,
 ' w': 25,
 ' x': 26,
 ' y': 27,
 ' z': 28,
 "' ": 29,
 "''": 30,
 "'.": 31,
 "'a": 32,
 "'b": 33,
 "'c": 34,
 "'d": 35,
 "'e": 36,
 "'f": 37,
 "'g": 38,
 "'h": 39,
 "'i": 40,
 "'j": 41,
 "'k": 42,
 "'l": 43,
 "'m": 44,
 "'n": 45,
 "'o": 46,
 "'p": 47,
 "'q": 48,
 "'r": 49,
 "'s": 50,
 "'t": 51,
 "'u": 52,
 "'v": 53,
 "'w": 54,
 "'x": 55,
 "'y": 56,
 "'z": 57,
 '. ': 58,
 ".'": 59,
 '..': 60,
 '.a': 61,
 '.b': 62,
 '.c': 63,
 '.d': 64,
 '.e': 65,
 '.f': 66,
 '.g': 67,
 '.h': 68,
 '.i': 69,
 '.j': 70,
 '.k': 71,
 '.l': 72,
 '.m': 73,
 '.n': 74,
 '.o': 75,
 '.p': 76,
 '.q': 77,
 '.r': 78,
 '.s': 79,
 '.t': 80,
 '.u': 81,
 '.v': 82,
 '.w': 83,
 '.x': 84,
 '.y': 85,
 '.z': 86,
 'a ': 87,
 "a'": 88,
 'a.': 89,
 'aa': 90,
 'ab': 91,
 'ac': 92,
 'ad': 93,
 'ae': 94,
 'af': 95,
 'ag': 96,
 'ah': 97,
 'ai': 98,
 'aj': 99,
 'ak': 100,
 'al': 101,
 'am': 102,
 'an': 103,
 'ao': 104,
 'ap': 105,
 'aq': 106,
 'ar': 107,
 'as': 108,
 'at': 109,
 'au': 110,
 'av': 111,
 'aw': 112,
 'ax': 113,
 'ay': 114,
 'az': 115,
 'b ': 116,
 "b'": 117,
 'b.': 118,
 'ba': 119,
 'bb': 120,
 'bc': 121,
 'bd': 122,
 'be': 123,
 'bf': 124,
 'bg': 125,
 'bh': 126,
 'bi': 127,
 'bj': 128,
 'bk': 129,
 'bl': 130,
 'bm': 131,
 'bn': 132,
 'bo': 133,
 'bp': 134,
 'bq': 135,
 'br': 136,
 'bs': 137,
 'bt': 138,
 'bu': 139,
 'bv': 140,
 'bw': 141,
 'bx': 142,
 'by': 143,
 'bz': 144,
 'c ': 145,
 "c'": 146,
 'c.': 147,
 'ca': 148,
 'cb': 149,
 'cc': 150,
 'cd': 151,
 'ce': 152,
 'cf': 153,
 'cg': 154,
 'ch': 155,
 'ci': 156,
 'cj': 157,
 'ck': 158,
 'cl': 159,
 'cm': 160,
 'cn': 161,
 'co': 162,
 'cp': 163,
 'cq': 164,
 'cr': 165,
 'cs': 166,
 'ct': 167,
 'cu': 168,
 'cv': 169,
 'cw': 170,
 'cx': 171,
 'cy': 172,
 'cz': 173,
 'd ': 174,
 "d'": 175,
 'd.': 176,
 'da': 177,
 'db': 178,
 'dc': 179,
 'dd': 180,
 'de': 181,
 'df': 182,
 'dg': 183,
 'dh': 184,
 'di': 185,
 'dj': 186,
 'dk': 187,
 'dl': 188,
 'dm': 189,
 'dn': 190,
 'do': 191,
 'dp': 192,
 'dq': 193,
 'dr': 194,
 'ds': 195,
 'dt': 196,
 'du': 197,
 'dv': 198,
 'dw': 199,
 'dx': 200,
 'dy': 201,
 'dz': 202,
 'e ': 203,
 "e'": 204,
 'e.': 205,
 'ea': 206,
 'eb': 207,
 'ec': 208,
 'ed': 209,
 'ee': 210,
 'ef': 211,
 'eg': 212,
 'eh': 213,
 'ei': 214,
 'ej': 215,
 'ek': 216,
 'el': 217,
 'em': 218,
 'en': 219,
 'eo': 220,
 'ep': 221,
 'eq': 222,
 'er': 223,
 'es': 224,
 'et': 225,
 'eu': 226,
 'ev': 227,
 'ew': 228,
 'ex': 229,
 'ey': 230,
 'ez': 231,
 'f ': 232,
 "f'": 233,
 'f.': 234,
 'fa': 235,
 'fb': 236,
 'fc': 237,
 'fd': 238,
 'fe': 239,
 'ff': 240,
 'fg': 241,
 'fh': 242,
 'fi': 243,
 'fj': 244,
 'fk': 245,
 'fl': 246,
 'fm': 247,
 'fn': 248,
 'fo': 249,
 'fp': 250,
 'fq': 251,
 'fr': 252,
 'fs': 253,
 'ft': 254,
 'fu': 255,
 'fv': 256,
 'fw': 257,
 'fx': 258,
 'fy': 259,
 'fz': 260,
 'g ': 261,
 "g'": 262,
 'g.': 263,
 'ga': 264,
 'gb': 265,
 'gc': 266,
 'gd': 267,
 'ge': 268,
 'gf': 269,
 'gg': 270,
 'gh': 271,
 'gi': 272,
 'gj': 273,
 'gk': 274,
 'gl': 275,
 'gm': 276,
 'gn': 277,
 'go': 278,
 'gp': 279,
 'gq': 280,
 'gr': 281,
 'gs': 282,
 'gt': 283,
 'gu': 284,
 'gv': 285,
 'gw': 286,
 'gx': 287,
 'gy': 288,
 'gz': 289,
 'h ': 290,
 "h'": 291,
 'h.': 292,
 'ha': 293,
 'hb': 294,
 'hc': 295,
 'hd': 296,
 'he': 297,
 'hf': 298,
 'hg': 299,
 'hh': 300,
 'hi': 301,
 'hj': 302,
 'hk': 303,
 'hl': 304,
 'hm': 305,
 'hn': 306,
 'ho': 307,
 'hp': 308,
 'hq': 309,
 'hr': 310,
 'hs': 311,
 'ht': 312,
 'hu': 313,
 'hv': 314,
 'hw': 315,
 'hx': 316,
 'hy': 317,
 'hz': 318,
 'i ': 319,
 "i'": 320,
 'i.': 321,
 'ia': 322,
 'ib': 323,
 'ic': 324,
 'id': 325,
 'ie': 326,
 'if': 327,
 'ig': 328,
 'ih': 329,
 'ii': 330,
 'ij': 331,
 'ik': 332,
 'il': 333,
 'im': 334,
 'in': 335,
 'io': 336,
 'ip': 337,
 'iq': 338,
 'ir': 339,
 'is': 340,
 'it': 341,
 'iu': 342,
 'iv': 343,
 'iw': 344,
 'ix': 345,
 'iy': 346,
 'iz': 347,
 'j ': 348,
 "j'": 349,
 'j.': 350,
 'ja': 351,
 'jb': 352,
 'jc': 353,
 'jd': 354,
 'je': 355,
 'jf': 356,
 'jg': 357,
 'jh': 358,
 'ji': 359,
 'jj': 360,
 'jk': 361,
 'jl': 362,
 'jm': 363,
 'jn': 364,
 'jo': 365,
 'jp': 366,
 'jq': 367,
 'jr': 368,
 'js': 369,
 'jt': 370,
 'ju': 371,
 'jv': 372,
 'jw': 373,
 'jx': 374,
 'jy': 375,
 'jz': 376,
 'k ': 377,
 "k'": 378,
 'k.': 379,
 'ka': 380,
 'kb': 381,
 'kc': 382,
 'kd': 383,
 'ke': 384,
 'kf': 385,
 'kg': 386,
 'kh': 387,
 'ki': 388,
 'kj': 389,
 'kk': 390,
 'kl': 391,
 'km': 392,
 'kn': 393,
 'ko': 394,
 'kp': 395,
 'kq': 396,
 'kr': 397,
 'ks': 398,
 'kt': 399,
 'ku': 400,
 'kv': 401,
 'kw': 402,
 'kx': 403,
 'ky': 404,
 'kz': 405,
 'l ': 406,
 "l'": 407,
 'l.': 408,
 'la': 409,
 'lb': 410,
 'lc': 411,
 'ld': 412,
 'le': 413,
 'lf': 414,
 'lg': 415,
 'lh': 416,
 'li': 417,
 'lj': 418,
 'lk': 419,
 'll': 420,
 'lm': 421,
 'ln': 422,
 'lo': 423,
 'lp': 424,
 'lq': 425,
 'lr': 426,
 'ls': 427,
 'lt': 428,
 'lu': 429,
 'lv': 430,
 'lw': 431,
 'lx': 432,
 'ly': 433,
 'lz': 434,
 'm ': 435,
 "m'": 436,
 'm.': 437,
 'ma': 438,
 'mb': 439,
 'mc': 440,
 'md': 441,
 'me': 442,
 'mf': 443,
 'mg': 444,
 'mh': 445,
 'mi': 446,
 'mj': 447,
 'mk': 448,
 'ml': 449,
 'mm': 450,
 'mn': 451,
 'mo': 452,
 'mp': 453,
 'mq': 454,
 'mr': 455,
 'ms': 456,
 'mt': 457,
 'mu': 458,
 'mv': 459,
 'mw': 460,
 'mx': 461,
 'my': 462,
 'mz': 463,
 'n ': 464,
 "n'": 465,
 'n.': 466,
 'na': 467,
 'nb': 468,
 'nc': 469,
 'nd': 470,
 'ne': 471,
 'nf': 472,
 'ng': 473,
 'nh': 474,
 'ni': 475,
 'nj': 476,
 'nk': 477,
 'nl': 478,
 'nm': 479,
 'nn': 480,
 'no': 481,
 'np': 482,
 'nq': 483,
 'nr': 484,
 'ns': 485,
 'nt': 486,
 'nu': 487,
 'nv': 488,
 'nw': 489,
 'nx': 490,
 'ny': 491,
 'nz': 492,
 'o ': 493,
 "o'": 494,
 'o.': 495,
 'oa': 496,
 'ob': 497,
 'oc': 498,
 'od': 499,
 'oe': 500,
 'of': 501,
 'og': 502,
 'oh': 503,
 'oi': 504,
 'oj': 505,
 'ok': 506,
 'ol': 507,
 'om': 508,
 'on': 509,
 'oo': 510,
 'op': 511,
 'oq': 512,
 'or': 513,
 'os': 514,
 'ot': 515,
 'ou': 516,
 'ov': 517,
 'ow': 518,
 'ox': 519,
 'oy': 520,
 'oz': 521,
 'p ': 522,
 "p'": 523,
 'p.': 524,
 'pa': 525,
 'pb': 526,
 'pc': 527,
 'pd': 528,
 'pe': 529,
 'pf': 530,
 'pg': 531,
 'ph': 532,
 'pi': 533,
 'pj': 534,
 'pk': 535,
 'pl': 536,
 'pm': 537,
 'pn': 538,
 'po': 539,
 'pp': 540,
 'pq': 541,
 'pr': 542,
 'ps': 543,
 'pt': 544,
 'pu': 545,
 'pv': 546,
 'pw': 547,
 'px': 548,
 'py': 549,
 'pz': 550,
 'q ': 551,
 "q'": 552,
 'q.': 553,
 'qa': 554,
 'qb': 555,
 'qc': 556,
 'qd': 557,
 'qe': 558,
 'qf': 559,
 'qg': 560,
 'qh': 561,
 'qi': 562,
 'qj': 563,
 'qk': 564,
 'ql': 565,
 'qm': 566,
 'qn': 567,
 'qo': 568,
 'qp': 569,
 'qq': 570,
 'qr': 571,
 'qs': 572,
 'qt': 573,
 'qu': 574,
 'qv': 575,
 'qw': 576,
 'qx': 577,
 'qy': 578,
 'qz': 579,
 'r ': 580,
 "r'": 581,
 'r.': 582,
 'ra': 583,
 'rb': 584,
 'rc': 585,
 'rd': 586,
 're': 587,
 'rf': 588,
 'rg': 589,
 'rh': 590,
 'ri': 591,
 'rj': 592,
 'rk': 593,
 'rl': 594,
 'rm': 595,
 'rn': 596,
 'ro': 597,
 'rp': 598,
 'rq': 599,
 'rr': 600,
 'rs': 601,
 'rt': 602,
 'ru': 603,
 'rv': 604,
 'rw': 605,
 'rx': 606,
 'ry': 607,
 'rz': 608,
 's ': 609,
 "s'": 610,
 's.': 611,
 'sa': 612,
 'sb': 613,
 'sc': 614,
 'sd': 615,
 'se': 616,
 'sf': 617,
 'sg': 618,
 'sh': 619,
 'si': 620,
 'sj': 621,
 'sk': 622,
 'sl': 623,
 'sm': 624,
 'sn': 625,
 'so': 626,
 'sp': 627,
 'sq': 628,
 'sr': 629,
 'ss': 630,
 'st': 631,
 'su': 632,
 'sv': 633,
 'sw': 634,
 'sx': 635,
 'sy': 636,
 'sz': 637,
 't ': 638,
 "t'": 639,
 't.': 640,
 'ta': 641,
 'tb': 642,
 'tc': 643,
 'td': 644,
 'te': 645,
 'tf': 646,
 'tg': 647,
 'th': 648,
 'ti': 649,
 'tj': 650,
 'tk': 651,
 'tl': 652,
 'tm': 653,
 'tn': 654,
 'to': 655,
 'tp': 656,
 'tq': 657,
 'tr': 658,
 'ts': 659,
 'tt': 660,
 'tu': 661,
 'tv': 662,
 'tw': 663,
 'tx': 664,
 'ty': 665,
 'tz': 666,
 'u ': 667,
 "u'": 668,
 'u.': 669,
 'ua': 670,
 'ub': 671,
 'uc': 672,
 'ud': 673,
 'ue': 674,
 'uf': 675,
 'ug': 676,
 'uh': 677,
 'ui': 678,
 'uj': 679,
 'uk': 680,
 'ul': 681,
 'um': 682,
 'un': 683,
 'uo': 684,
 'up': 685,
 'uq': 686,
 'ur': 687,
 'us': 688,
 'ut': 689,
 'uu': 690,
 'uv': 691,
 'uw': 692,
 'ux': 693,
 'uy': 694,
 'uz': 695,
 'v ': 696,
 "v'": 697,
 'v.': 698,
 'va': 699,
 'vb': 700,
 'vc': 701,
 'vd': 702,
 've': 703,
 'vf': 704,
 'vg': 705,
 'vh': 706,
 'vi': 707,
 'vj': 708,
 'vk': 709,
 'vl': 710,
 'vm': 711,
 'vn': 712,
 'vo': 713,
 'vp': 714,
 'vq': 715,
 'vr': 716,
 'vs': 717,
 'vt': 718,
 'vu': 719,
 'vv': 720,
 'vw': 721,
 'vx': 722,
 'vy': 723,
 'vz': 724,
 'w ': 725,
 "w'": 726,
 'w.': 727,
 'wa': 728,
 'wb': 729,
 'wc': 730,
 'wd': 731,
 'we': 732,
 'wf': 733,
 'wg': 734,
 'wh': 735,
 'wi': 736,
 'wj': 737,
 'wk': 738,
 'wl': 739,
 'wm': 740,
 'wn': 741,
 'wo': 742,
 'wp': 743,
 'wq': 744,
 'wr': 745,
 'ws': 746,
 'wt': 747,
 'wu': 748,
 'wv': 749,
 'ww': 750,
 'wx': 751,
 'wy': 752,
 'wz': 753,
 'x ': 754,
 "x'": 755,
 'x.': 756,
 'xa': 757,
 'xb': 758,
 'xc': 759,
 'xd': 760,
 'xe': 761,
 'xf': 762,
 'xg': 763,
 'xh': 764,
 'xi': 765,
 'xj': 766,
 'xk': 767,
 'xl': 768,
 'xm': 769,
 'xn': 770,
 'xo': 771,
 'xp': 772,
 'xq': 773,
 'xr': 774,
 'xs': 775,
 'xt': 776,
 'xu': 777,
 'xv': 778,
 'xw': 779,
 'xx': 780,
 'xy': 781,
 'xz': 782,
 'y ': 783,
 "y'": 784,
 'y.': 785,
 'ya': 786,
 'yb': 787,
 'yc': 788,
 'yd': 789,
 'ye': 790,
 'yf': 791,
 'yg': 792,
 'yh': 793,
 'yi': 794,
 'yj': 795,
 'yk': 796,
 'yl': 797,
 'ym': 798,
 'yn': 799,
 'yo': 800,
 'yp': 801,
 'yq': 802,
 'yr': 803,
 'ys': 804,
 'yt': 805,
 'yu': 806,
 'yv': 807,
 'yw': 808,
 'yx': 809,
 'yy': 810,
 'yz': 811,
 'z ': 812,
 "z'": 813,
 'z.': 814,
 'za': 815,
 'zb': 816,
 'zc': 817,
 'zd': 818,
 'ze': 819,
 'zf': 820,
 'zg': 821,
 'zh': 822,
 'zi': 823,
 'zj': 824,
 'zk': 825,
 'zl': 826,
 'zm': 827,
 'zn': 828,
 'zo': 829,
 'zp': 830,
 'zq': 831,
 'zr': 832,
 'zs': 833,
 'zt': 834,
 'zu': 835,
 'zv': 836,
 'zw': 837,
 'zx': 838,
 'zy': 839,
 'zz': 840}</code></pre>
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/yangrchen\.github\.io\/cracking-data-yang\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/yangrchen/cracking-data-yang/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div></div></footer></body></html>