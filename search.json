[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Cracking Data is all about my love for experimenting with data. It’s a collection of written projects which help explain awesome data tools and concepts disciplines that I am passionate about like art!\nWho am I? I’m a data science Master’s student at the Institute for Advanced Analytics who’s interested in AI, deep learning, and explainable data science. I aim to keep my work fun and informative so others might be inspired to share their own experiments.\nTo give feedback, ask questions, or share funny stories you can find me on LinkedIn or email rchen28@ncsu.edu."
  },
  {
    "objectID": "posts/projects/artwork_classifier/index.html",
    "href": "posts/projects/artwork_classifier/index.html",
    "title": "Classifying Artwork Types With Fastai",
    "section": "",
    "text": "I’ve always been interested in how machine learning can apply to visual arts. In particular, computer vision feels like the most direct application to art since we can build programs to classify, modify, and even generate visuals in an artistic fashion!\nI’m by no means a deep learning aficionado but it’s exciting to tinker around with the different tools and concepts I have at my disposal. In this notebook, we will be exploring how to build out an image classification model which classifies artwork based on their different types.\nWe’ll be using the fastai framework to speed up the development process and utilize modern deep learning techniques. Along the way, I will also be covering several different concepts that I have found useful in implementing vision models. I assume some level of familiarity with machine learning concepts such as epochs and mini-batch gradient descent. And ultimately, my hope is for this notebook to become the start of projects that inspire and help other deep learning practitioners along their own journeys!"
  },
  {
    "objectID": "posts/projects/artwork_classifier/index.html#dataloader-function",
    "href": "posts/projects/artwork_classifier/index.html#dataloader-function",
    "title": "Classifying Artwork Types With Fastai",
    "section": "4.1 Dataloader Function",
    "text": "4.1 Dataloader Function\n\n\nCode\ndef get_dls(bs: int, size: int, valid_pct=0.2, seed=1234) -&gt; DataLoaders:\n    \"\"\"\n    Creates a dataloader for an image recognition task with specified batch size and image size.\n\n    # Parameters:\n    -   bs (int): Batch size.\n    -   size (int): Final size of individual images.\n\n    # Returns:\n    -   DataLoaders: A dataloader that batches our image data with specific image and batch size.\n    \"\"\"\n    db = DataBlock(\n        blocks=(ImageBlock(), CategoryBlock()),\n        get_items=get_image_files,\n        splitter=RandomSplitter(valid_pct=valid_pct, seed=seed), \n        get_y=parent_label,\n        item_tfms=Resize(300),\n        batch_tfms=aug_transforms(size=size, min_scale=0.75)\n    )\n    return db.dataloaders(data_path / 'training_set', bs=bs, seed=1234)\n\n\nWhen we create our dataloader, we can verify that our splitter is working as we expect it to.\n\n\nCode\nset_seed(SEED, reproducible=True)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\ndls = get_dls(64, 128, seed=SEED)\n\nlen(dls.train_ds), len(dls.valid_ds)\n\n\n(6177, 1544)\n\n\nNow, let’s check out what images we have to work with using the show_batch function!\n\n\nCode\n# Show an example batch of images\ndls.show_batch()"
  },
  {
    "objectID": "posts/projects/artwork_classifier/index.html#learning-rate-finder",
    "href": "posts/projects/artwork_classifier/index.html#learning-rate-finder",
    "title": "Classifying Artwork Types With Fastai",
    "section": "5.1 Learning Rate Finder",
    "text": "5.1 Learning Rate Finder\nFor our model, we first need to select a good learning rate to start our training. Fastai implements a learning rate finder method Learner.lr_find based on Leslie Smith’s paper called Cyclical Learning Rates for Training Neural Networks.\nSmith describes his approach as the following:\n\nThere is a simple way to estimate reasonable minimum and maximum boundary values with one training run of the network for a few epochs. It is a “LR range test”; run your model for several epochs while letting the learning rate increase linearly between low and high LR values.\n\nEssentially, we check our training losses as we grow our learning rate to get an idea of what learning rate performs well.\nFastai takes a slightly different approach: The model is test-trained with learning rates that grow exponentially from a low learning rate to a higher learning rate across a number of mini-batches. The process continues until we find a learning rate where the loss begins to diverge and increases significantly.\nWhat this boils down to is that in the fastai version we don’t necessarily need to run for multiple epochs to get an optimal learning rate as we are training for a set number of mini-batches.\nConveniently, lr_find gives us the loss vs. learning rate plot and also enables us to directly extract learning rates that could be optimal based off different suggestion functions. For example, we can use steep which gets the learning rate with the steepest slope.\n\n\nCode\nset_seed(SEED, reproducible=True)\nlr_steep = learn.lr_find(suggest_funcs=steep)\n\n\n\nLog plot of loss vs. learning rate"
  },
  {
    "objectID": "posts/projects/artwork_classifier/index.html#one-cycle-idea",
    "href": "posts/projects/artwork_classifier/index.html#one-cycle-idea",
    "title": "Classifying Artwork Types With Fastai",
    "section": "5.2 One-cycle Idea",
    "text": "5.2 One-cycle Idea\nWe can now start training our model! Using the fit_one_cycle method we are actually training our model with what is called the one-cycle policy, an idea that again comes from Leslie Smith and changes our learning rate over the course of training. I won’t be going into close detail on how the 1cycle policy is implemented in the scope of this article.\nAt a high level, we are starting at some initial learning rate, linearly increasing our learning rate after every batch up to a maximum learning rate, and from the maximum learning rate down to some minimum learning rate several magnitudes lower than our initial learning rate.\nThe idea behind this all is to warm up our training with a low learning rate and use the high learning rate to help find minimums in our loss function that are flatter, allowing the model to generalize better.\nDuring the last segment of training, the descending learning rates help the optimizer avoid skipping over a steeper loss within the flatter areas. This process allows our model to converge faster and consequently achieve better results with lower iterations than traditional training methods. Smith calls this occurrence super-convergence.\n\n\nCode\ndef plot_example_cycle(x_min, x_max, y_min, y_max, padding):\n    values = 100\n    x = np.linspace(x_min, x_max, values + padding)\n\n    y1 = np.linspace(y_min, y_max, values // 2, endpoint=False)\n    y2 = np.linspace(y_max, y_min, values // 2, endpoint=False)\n    padded_values = np.linspace(y_min, y_min * 1e-2, padding)\n\n    y = np.concatenate((y1, y2, padded_values))\n\n    plt.plot(x, y)\n    plt.xlabel('Epochs')\n    plt.ylabel('Learning rate')\n    plt.show()\n\n\n\n\nCode\nplot_example_cycle(0, 40, 0.001, 0.01, 15)\n\n\n\n\n\nFigure 1: Example representation of a linear 1cycle learning rate schedule.\n\n\n\n\n\nWith transfer learning, our model has additional new layers that we can train for the problem we are trying to solve. At the start of training, we may not want to completely readjust the weights in the model’s previously learned layers because they account for high-level details of the image like line and shape.\nIn a pretrained model, the previous trained layers start out frozen, meaning their weights aren’t updated during training unless we unfreeze them. Later, we will unfreeze all the layers and use a range of learning rates to help adjust them slightly for our artwork classification.\nFastai recommends training the frozen pretrained model for a few epochs before training the full pretrained model. We’ll start with 3 epochs frozen using the learning rate we found before and then train the full model for 10 epochs. We can get an idea of how our error rates changes across training from here. Our losses along with our metrics will be useful in determining if we’ve fitted a decent model or whether we are underfitting or overfitting.\n\n\nCode\nset_seed(SEED, reproducible=True)\nlearn.fit_one_cycle(3, lr_steep.steep)\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.736086\n0.369670\n0.887953\n00:08\n\n\n1\n0.441399\n0.284378\n0.909326\n00:08\n\n\n2\n0.321099\n0.254835\n0.911269\n00:08\n\n\n\n\n\nNow we can unfreeze the layers and find a new learning rate to train on:\n\n\nCode\nset_seed(SEED, reproducible=True)\nlearn.unfreeze()\nlearn.lr_find()\n\n\n\n\n\nSuggestedLRs(valley=0.00013182566908653826)\n\n\n\n\n\n\n\nCode\nset_seed(SEED, reproducible=True)\nlearn.fit_one_cycle(10, 1e-5)\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.290775\n0.243898\n0.915155\n00:09\n\n\n1\n0.292668\n0.236558\n0.915155\n00:09\n\n\n2\n0.248140\n0.215108\n0.920984\n00:09\n\n\n3\n0.219748\n0.209778\n0.926166\n00:09\n\n\n4\n0.192568\n0.201274\n0.928109\n00:09\n\n\n5\n0.186713\n0.200779\n0.926813\n00:09\n\n\n6\n0.169397\n0.188377\n0.929404\n00:09\n\n\n7\n0.157394\n0.192167\n0.931347\n00:09\n\n\n8\n0.146395\n0.195563\n0.928109\n00:09\n\n\n9\n0.159019\n0.191388\n0.931995\n00:09"
  },
  {
    "objectID": "posts/projects/artwork_classifier/index.html#interpreting-the-loss-curves-further-training",
    "href": "posts/projects/artwork_classifier/index.html#interpreting-the-loss-curves-further-training",
    "title": "Classifying Artwork Types With Fastai",
    "section": "5.3 Interpreting the Loss Curves, Further Training?",
    "text": "5.3 Interpreting the Loss Curves, Further Training?\nAfter training our model, we will see how our training and validation losses have changed over the course of our iteration / epochs using loss curves. Loss curves won’t provide the entire story of our model but we’ll have a broad picture of how our model performs over the selected dataset and batch size. Figure 2 shows the loss curves across the training iterations.\n\n\nCode\nlearn.recorder.plot_loss()\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\nplt.title('Loss Curves Across Iterations')\nplt.show()\n\n\n\n\n\nFigure 2: Loss curves across the model’s training iterations\n\n\n\n\nInitially, our validation loss starts out lower than our training loss but our training loss quickly converges as our training progresses.\nWe should note that we reach a point where our validation loss is higher than our training loss and stagnates a bit. The model is becoming overconfident in its predictions and we have to ask ourselves: are we overfitting?\nFigure 3 shows the loss curves across the training epochs in case we need to go back and retrain our model to a previous epoch.\n\n\nCode\ntrain_loss = L(learn.recorder.values).itemgot(0)\nvalid_loss = L(learn.recorder.values).itemgot(1)\n\nplt.plot(train_loss)\nplt.plot(valid_loss)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Loss Curves Across Epochs')\nplt.show()\n\n\n\n\n\nFigure 3: Loss curves across training epochs"
  },
  {
    "objectID": "posts/projects/artwork_classifier/index.html#overfitting",
    "href": "posts/projects/artwork_classifier/index.html#overfitting",
    "title": "Classifying Artwork Types With Fastai",
    "section": "5.4 Overfitting",
    "text": "5.4 Overfitting\nOverfitting occurs when we have trained for too long and the model begins to “memorize” the training data while failing to generalize well to new data.\nWhen we look at our losses visually, we can see that there is an emerging gap between our training and validation loss and the latter seems to stagnate. How big of a gap is too big? It’s hard to say because it depends on a variety of factors like the scale of the dataset. Rather, we should look for general stability in our validation loss throughout training. If our validation loss begins to increase significantly, then we might begin to worry that we are overfitting to our training data too much.\nWe could stop early, but with one-cycle training that may not be a good idea because we may not allow our learning rate to reach the small values that would benefit our training.\nOne key takeaway from fastai is that we should be checking to see if our performance metrics are getting significantly worse to decide if the model is overfitting. It’s not enough to view the losses alone.\nFigure 4 shows the accuracy across the training epochs.\n\n\nCode\naccuracy_metric = L(learn.recorder.values).itemgot(2)\nhighest_accuracy_epoch = np.argmax(accuracy_metric)\nprint(f'Highest accuracy during last run: {highest_accuracy_epoch}')\n\nplt.xlabel('Accuracy')\nplt.ylabel('Epoch')\nplt.title('Accuracy Across Epochs')\nplt.xticks(np.arange(0, len(accuracy_metric) + 1, 5))\n\nplt.plot(accuracy_metric)\nplt.scatter(highest_accuracy_epoch, accuracy_metric[highest_accuracy_epoch], color='orange')\nplt.show()\n\n\nHighest accuracy during last run: 9\n\n\n\n\n\nFigure 4: Accuracy across the training epochs\n\n\n\n\nOur highest accuracy is at epoch 9 but the accuracy appears to be flattening out towards the end. If we take into account both our validation loss and metric, then we aren’t too concerned about overfitting. Fortunately, we do have a few options in the case of severe overfitting! We could introduce weight decay or even rerun our model with a lower number of epochs and adjust from there. In this case, we will add some weight decay using discriminative learning rates.\nWe should keep in mind that our performance metric is what ultimately matters in practice.\nAs Jeremy Howard states:\n\n“In the end what matters is your accuracy, or more generally your chosen metrics, not the loss. The loss is just the function we’ve given the computer to help us to optimize.”\n“Remember, it’s not just that we’re looking for the validation loss to get worse, but the actual metrics. Your validation loss will first get worse during training because the model gets overconfident, and only later will get worse because it is incorrectly memorizing the data. We only care in practice about the latter issue. Remember, our loss function is just something that we use to allow our optimizer to have something it can differentiate and optimize; it’s not actually the thing we care about in practice.”\n\nTo really evaluate our model’s performance, we would run our model on a representative test data set that it has never seen before. This will allow us to get a more honest assessment of how our model is doing and we don’t want to report accuracy on our training model alone. Once we’ve decided on a model, we’ll look at how we can use the images we didn’t use for training as a test set.\n\n\nCode\nset_seed(SEED, reproducible=True)\n\n# Reinitialize our model to restart training\nlearn2 = vision_learner(dls, resnet50, metrics=accuracy, wd=0.1)\n\n\n\n\nCode\nset_seed(SEED, reproducible=True)\nlr_steep = learn2.lr_find(suggest_funcs=steep, show_plot=False)\n\n\n\n\n\n\n\nCode\nset_seed(SEED, reproducible=True)\nlearn2.fit_one_cycle(3, lr_steep.steep)\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.737313\n0.343379\n0.890544\n00:08\n\n\n1\n0.418140\n0.294040\n0.908679\n00:08\n\n\n2\n0.324219\n0.272586\n0.914508\n00:08\n\n\n\n\n\n\n\nCode\nset_seed(SEED, reproducible=True)\nlearn2.unfreeze()\nlearn2.lr_find()\n\n\n\n\n\nSuggestedLRs(valley=6.30957365501672e-05)\n\n\n\n\n\n\n\nCode\nset_seed(SEED, reproducible=True)\nlearn2.fit_one_cycle(10, lr_max=slice(1e-6, 1e-4))\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.284886\n0.262790\n0.918394\n00:09\n\n\n1\n0.272633\n0.263987\n0.917746\n00:09\n\n\n2\n0.253378\n0.242250\n0.922280\n00:10\n\n\n3\n0.223389\n0.225795\n0.930052\n00:09\n\n\n4\n0.204689\n0.231059\n0.920984\n00:09\n\n\n5\n0.170040\n0.227536\n0.925518\n00:10\n\n\n6\n0.153856\n0.218628\n0.928756\n00:09\n\n\n7\n0.148347\n0.223874\n0.927461\n00:09\n\n\n8\n0.140296\n0.223210\n0.930699\n00:09\n\n\n9\n0.136215\n0.218119\n0.930699\n00:10\n\n\n\n\n\nOur accuracy is about the same, but both validation losses seemed to stabilize. The validation loss and accuracy are better in our first model, so we will move forward with it for now.\n\n\nCode\nlearn2.recorder.plot_loss()\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\nplt.title('Loss Curves Across Iterations')\nplt.show()\n\n\n\n\n\nFigure 5: Loss curves across the model’s training iterations\n\n\n\n\n\n\nCode\ntrain_loss = L(learn2.recorder.values).itemgot(0)\nvalid_loss = L(learn2.recorder.values).itemgot(1)\n\nplt.plot(train_loss)\nplt.plot(valid_loss)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Loss Curves Across Epochs')\nplt.show()\n\n\n\n\n\nFigure 6: Loss curves across training epochs\n\n\n\n\n\n\nCode\naccuracy_metric = L(learn2.recorder.values).itemgot(2)\nhighest_accuracy_epoch = np.argmax(accuracy_metric)\nprint(f'Highest accuracy during last run: {highest_accuracy_epoch}')\n\nplt.xlabel('Accuracy')\nplt.ylabel('Epoch')\nplt.title('Accuracy Across Epochs')\nplt.xticks(np.arange(0, len(accuracy_metric) + 1, 5))\n\nplt.plot(accuracy_metric)\nplt.scatter(highest_accuracy_epoch, accuracy_metric[highest_accuracy_epoch])\nplt.show()\n\n\nHighest accuracy during last run: 8\n\n\n\n\n\nFigure 7: Accuracy across the training epochs\n\n\n\n\nWe can use a confusion matrix to see the number of correctly classified and misclassified images on our training set. Figure 8 displays the confusion matrix for the first model we fine-tuned.\n\n\n\nCode\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n(a) Confusion matrix on the training set\n\n\n\n\n\n\n(b)\n\n\n\nFigure 8: ?(caption)\n\n\nWe can also display a report with common classification metrics like precision, recall, and the F1-score. For this project, we are mainly focused on the F1-score.\nThe weighted F1-score is the the mean of all the class F1 scores while taking into account the number of occurrences in each class. In general, we are aiming for a balance between precision and recall on a scale between 0 and 1. Our model achieved a weighted F1-score of 0.930 on the training set.\n\n\nCode\ninterp.print_classification_report()\n\n\n\n\n\n              precision    recall  f1-score   support\n\n    drawings       0.83      0.75      0.79       208\n   engraving       0.78      0.86      0.82       161\n iconography       0.97      1.00      0.98       419\n    painting       0.98      0.94      0.96       410\n   sculpture       0.96      0.99      0.97       346\n\n    accuracy                           0.93      1544\n   macro avg       0.90      0.91      0.90      1544\nweighted avg       0.93      0.93      0.93      1544\n\n\n\nFigure 9 shows the top 10 losses of training images predicted by our model. We can use this to get a visual idea of what kinds of images our model might be misclassifying.\nIn particular, from both the classification report and the top losses our model seems to misclassify engravings and drawings more than other types of art. We also see some confusion between iconography and paintings. Maybe not all images are standardized and there are definitely tough cases where the line between painting, drawing, and iconography are blurry.\n\n\n\nCode\ninterp.plot_top_losses(10)\n\n\n\n\n(a) Top 10 losses of images in the training set\n\n\n\n\n\n\n(b)\n\n\n\nFigure 9: ?(caption)\n\n\nIt’s fascinating that we are able to classify images at all with these results with a bit of conceptual understanding and a few lines of code! However, we still need to evaluate on the test set for reporting purposes."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cracking Data with Yang",
    "section": "",
    "text": "Classifying Artwork Types With Fastai\n\n\n\n\n\n\n\nproject\n\n\nmachine learning\n\n\ncomputer vision\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2023\n\n\nYang Chen\n\n\n\n\n\n\nNo matching items"
  }
]