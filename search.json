[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Cracking Data is all about my love for experimenting with data. It’s a collection of written projects which help explain awesome data tools and concepts disciplines that I am passionate about like art!\nWho am I? I’m a data science Master’s student at the Institute for Advanced Analytics who’s interested in AI, deep learning, and explainable data science. I aim to keep my work fun and informative so others might be inspired to share their own experiments.\nTo give feedback, ask questions, or share funny stories you can find me on LinkedIn or email rchen28@ncsu.edu."
  },
  {
    "objectID": "posts/projects/fantasy_makemore/index.html",
    "href": "posts/projects/fantasy_makemore/index.html",
    "title": "Exploring Karpathy’s Makemore Concepts Using Fantasy DnD Names: Part 1",
    "section": "",
    "text": "I dedicated a month to understanding the mechanics of language modeling—without too much of the marketing hype attached with the popularity of large language models. During my studies, I came across Andrej Karpathy’s makemore video series. He progressively builds up the concepts and code that lead to generative pre-training transformer (GPT) models starting with character-level language models.\nTo say the least, this video series is a remarkably detailed set of lectures that really help demystify language modeling all the way up through deep learning concepts. I wanted to try my hand at applying this awesome work on a fun dataset that I’ve curated myself—DnD fantasy names (the amazing drow)! I like Karpathy’s advice on “creating cheatsheets” too, so this post is my personal (large) cheatsheet that hopefully helps beginners like me on their own journey.\nThis project will be broken up into 4 parts:\n\nPart 1 will cover scraping the necessary data, creating a counting bigram model, understanding negative log-likelihood loss, and scaling up the model as a neural net.\nPart 2 is all about using embeddings and creating a regular feedforward neural net to generate names. We will create our train/val/test split and experiment with some settings to improve our performance.\nPart 3 will expand on the feedforward net from Part 2, diving into better weight initializations and BatchNorm to stabilize the model’s performance.\nPart 4 is TBD! But I’d like to build a transformer on fantasy dialogue."
  },
  {
    "objectID": "posts/projects/fantasy_makemore/index.html#generating-characters-through-sampling",
    "href": "posts/projects/fantasy_makemore/index.html#generating-characters-through-sampling",
    "title": "Exploring Karpathy’s Makemore Concepts Using Fantasy DnD Names: Part 1",
    "section": "Generating Characters Through Sampling",
    "text": "Generating Characters Through Sampling\nWith our frequencies in place, we can start generating names! Intuitively, row 0 represents the frequencies of every other character following the . character. So if we want to find the probability of each character following . then we just have to divide each row entry by the sum of all the row frequencies. We now have a probability distribution for characters following .\nWe’ll do the same for every other row and SHAZAM… we have a probability matrix we can sample from.\n\nP = N.float()\nP /= P.sum(\n    dim=1, keepdim=True\n)  # By keeping the row dimension, the row sum array will be broadcasted across the rest of P\nassert torch.allclose(\n    torch.ones(P.shape[0], dtype=torch.float32), P.sum(dim=1)\n)  # Check to see that every row sums to 1 in our probability matrix\n\n\ng = torch.Generator().manual_seed(1234)  # Reproducibility\nfor i in range(10):\n    out = []\n    ix = 0  # Start with the `.` character\n    while True:\n        p = P[ix]\n        ix = torch.multinomial(\n            p, num_samples=1, replacement=True, generator=g\n        ).item()  # Sample from our probability matrix and get back an index which represents the new character\n\n        if ix == 0:  # Terminate if we end up back on the `.` char\n            break\n        out.append(itos[ix])  # Append the decoded character to our output\n    print(\"\".join(out))\n\nirorelelatodele sonzmr\nsd\nrn kho'gorlavzae\nh drinan s\nclorhimllan dhelolaarlahimanin aun mtathan\nileryn jhyol'tont\nthes cenodaelar zabh\noltenn zalarlyr xl'lan drar alrathurilaly\nzamvadrylmdal zekrazarifar\nduendalaurrilelyaeanina'auicod\n\n\nI’d love to see a DnD character with the name duendalaurrilelyaeanina'auicod. Unfortunately, I don’t know any that exist yet. There are definitely some names that sound like they could be in DnD like ileryn and zekrazarifar, but also we can see some strange artifacts like sd and rn hanging around.\nCompared to typical English names, it can be difficult to verify how good the output is or even if the bigram model is working as expected. One sanity check is to make every character have uniform probability—every character is equally likely to be sampled.\n\ng = torch.Generator().manual_seed(1234)  # Reproducibility\nfor i in range(10):\n    out = []\n    ix = 0  # Start with the `.` character\n    while True:\n        # p = P[ix]\n        p = torch.ones(vocab_size) / vocab_size  # Uniform distribution\n        ix = torch.multinomial(\n            p, num_samples=1, replacement=True, generator=g\n        ).item()  # Sample from our probability matrix and get back an index which represents the new character\n\n        if ix == 0:  # Terminate if we end up back on the `.` char\n            break\n        out.append(itos[ix])  # Append the decoded character to our output\n    print(\"\".join(out))\n\nwxoorlbqjaodxlfwsoizmjzsd'rlovhj'govlnvzqpchkdlijvfqswc\noyhimsljayrhnjxlaawlmhifanicgempzuhatngychlmwffvjdm'u'fkdsftyfsdc\nsuplhfqkrzwpfpxlf'ezpz'lqrlpfnxc'cqqcdbac airmqhugilnkr\nzbmvpdrtbmdyf ze raw'ipfo xduccyaqrimwuieqwpeaosca'ndiccd\n'tzbyl lyhzbzchnfqtjdcwzyrhy q klmsod yafxwcbwziqu'mtdhhycrjpmjdmhcnowtcdzffadoebpodazgqqjswqudquwanz'ncaktcxk\nmqulrvhoez\nkupksskhynn\nl efaqjvcrpljcxqn\nrayi\nz\n\n\nAll hail the demonic wxoorlbqjaodxlfwsoizmjzsd'rlovhj'govlnvzqpchkdlijvfqswc. Your soul will be split in half just trying to pronounce that. It definitely seems like the bigram model is working to an extent.\nHowever, we can still have a bit more confidence by having a method to summarize our model numerically."
  },
  {
    "objectID": "posts/projects/fantasy_makemore/index.html#generating-characters-for-the-trigram-model",
    "href": "posts/projects/fantasy_makemore/index.html#generating-characters-for-the-trigram-model",
    "title": "Exploring Karpathy’s Makemore Concepts Using Fantasy DnD Names: Part 1",
    "section": "Generating Characters for the Trigram Model",
    "text": "Generating Characters for the Trigram Model\nNow we can generate some characters using the same method from the bigram model. I’m curious what names we’ll get if we start with the first letter of my own name.\n\nP = (N + 1).float()  # Model smoothing\nP /= P.sum(\n    dim=-1, keepdim=True\n)  # Normalize the counts along the last dimension to be a probability distribution\n\ng = torch.Generator().manual_seed(1234)  # Reproducibility\nfor i in range(15):\n    out = [\"y\"]\n    ix1, ix2 = stoi[\".\"], stoi[\"y\"]  # Start with \".y\"\n    while True:\n        p = P[ix1, ix2]  # Get the probability for current bigram\n        ix1 = ix2  # Shift the \"context window\" by 1\n        ix2 = torch.multinomial(\n            p, num_samples=1, replacement=True, generator=g\n        ).item()  # Get the next predicted character\n        if ix2 == 0:\n            break\n        out.append(itos[ix2])\n    print(\"\".join(out))  # Print the final result\n\nyaundros alodeafeinimmelddarn khalgovanvar chadrizza\nyurzena\nyauluan dhund\nyaamlespmaerhrammyrrath\nyaunzenelvid dus ket\nyaufein\nyaudaen kryl\nyazzath\nyatanlyrin tel\nyaan drae\nyaurae\nyuillanorzzavaddtlimas zak al\nyaufeindaendalauvryle helani\nyazza\nyund\n\n\nAnd now when we evaluate the trigram model loss:\n\nlog_likelihood = 0.0\nn_ele = 0.0\n\nfor n in names:\n    chs = [\".\", *n, \".\"]\n    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n        ix1, ix2, ix3 = stoi[ch1], stoi[ch2], stoi[ch3]\n        log_likelihood += torch.log(P[ix1, ix2, ix3])\n        n_ele += 1\n\nnll = -log_likelihood\nprint(f\"{nll = }\")\nprint(f\"{nll/n_ele = }\")\n\nnll = tensor(1523841.2500)\nnll/n_ele = tensor(1.8337)\n\n\nThe trigram model seems to perform better than the bigram model on the full data and we love ourselves a goblin-sounding yazza. Overall, n-gram models are straightforward and they do a good job of optimizing our loss function in this setting.\nHowever, a more flexible framework is to use gradient-based optimization in a neural net. An overview: Instead of explicitly counting characters, we can tune weights with gradient descent that, multiplied by our inputs, will create logits. These predicted logits will then be transformed into a probability distribution via softmax which can be used to predict the next token. As Karpathy mentions, the simple neural net we are creating will ultimately get us to a similar place as the counting approach, but we will be able to expand on the architecture much further than the counting models in later parts."
  },
  {
    "objectID": "posts/projects/fantasy_makemore/index.html#bigram-model",
    "href": "posts/projects/fantasy_makemore/index.html#bigram-model",
    "title": "Exploring Karpathy’s Makemore Concepts Using Fantasy DnD Names: Part 1",
    "section": "Bigram Model",
    "text": "Bigram Model\n\nX, y = [], []\n\nfor n in names:\n    chs = [\".\", *n, \".\"]  # Pad each name with the terminating chars\n    for ch1, ch2 in zip(chs, chs[1:]):\n        ix1, ix2 = stoi[ch1], stoi[ch2]\n        X.append(ix1)\n        y.append(ix2)\n\nX = torch.tensor(X)\ny = torch.tensor(y)\nnum_X = X.nelement()\nprint(\"Number of examples:\", num_X)\n\nNumber of examples: 883744\n\n\n\nX, y\n\n(tensor([ 0, 13, 20,  ...,  2, 22, 10]),\n tensor([13, 20,  7,  ..., 22, 10,  0]))\n\n\nThe second step is to feed our character indices into the model. However, we don’t want to directly feed in the integer values. For each index, we’d like to select the corresponding row in the weights matrix through matrix multiplication.\nThis is where one-hot encoding comes in! Each integer index \\(i\\) can be represented as a vector that is all 0s except for the \\(i\\) position.\n\\[\n\\left[\n    \\begin{matrix}\n    1 & 2 & 3 & 4 \\\\\n    \\end{matrix}\n\\right]\n\\longrightarrow\n\\left[\n    \\begin{matrix}\n    1 & 0 & 0 & 0 \\\\\n    0 & 1 & 0 & 0 \\\\\n    0 & 0 & 1 & 0 \\\\\n    0 & 0 & 0 & 1\n    \\end{matrix}\n\\right]\n\\]\nWhen multiplied with our randomly initialized weight matrix \\(W\\), the one-hot encoded vectors will select the corresponding rows in \\(W\\) similar to how we selected the rows from our probability matrix during character generation.\n\nimport torch.nn.functional as F\n\ng = torch.Generator().manual_seed(1234)\n\nx_enc = F.one_hot(X, num_classes=len(stoi)).float()  # One-hot encode our input indices\nW = torch.randn(\n    (len(stoi), len(stoi)), dtype=torch.float32, generator=g\n)  # Initialize our weight matrix with random Normal values, shape (29, 29)\n\n\n# Demonstrating the row selection with one-hot vectors\nfirst_row = F.one_hot(torch.tensor(0), num_classes=len(stoi)).float()\nprint(\n    \"Test one-hot vector selects first row?\", torch.all(W[0] == (first_row @ W)).item()\n)\n\nTest one-hot vector selects first row? True\n\n\n\nlogits = x_enc @ W  # Linear layer outputs log-counts / logits for all observations in X\nlogits\n\ntensor([[-0.1117, -0.4966,  0.1631,  ...,  0.2207,  0.2463, -1.3248],\n        [-1.4317,  2.1155, -1.1853,  ...,  1.4487, -1.5155, -1.2364],\n        [ 1.4007, -1.0216, -0.5113,  ...,  0.0163,  0.0612, -2.6981],\n        ...,\n        [ 0.6808,  0.7244,  0.0323,  ...,  0.4308,  0.2862, -0.2481],\n        [ 1.2134,  0.8028, -0.4904,  ..., -0.2103, -0.9933, -0.2486],\n        [ 0.6091,  0.6668, -1.1206,  ..., -0.9728, -1.2452, -0.0115]])\n\n\nBy multiplying our one-hot encoded inputs with our weights, we produce log-counts / logits. Logits are raw, unnormalized scores that the model outputs typically before applying some activation function.\nTaking a quick glance at our logits, these can’t be probabilities! We have a bunch of positive and negative numbers, some of which are greater than 1. To represent probabilities, the sum across the columns should be 1.\nWe’d like to perform an operation similar to the sigmoid function which transforms a real number to be between 0 and 1, but this time the sum across a vector of values should be 1. This is exactly what softmax is: a generalization of sigmoid function to multiple dimensions!\nWe normalize our logits into a probability distribution using softmax. All softmax does is it divides an exponentiated input vector by the sum of all its exponentiated values.\n\\[\n\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}\n\\]\nThe sum in the denominator mirrors what we did with the character counts previously (integers, greater than or equal to 0). There we just divided every count by the sum of the counts across the columns.\nIf you think about it, softmax gives us a good representation of what we’re trying to do in multiclass classification where we’d like to utilize a probability space across all the classes after we train our model.\n\ncounts = logits.exp()  # Exponentiate the logits\nprobs = counts / counts.sum(dim=1, keepdim=True)\nassert torch.allclose(torch.ones(probs.shape[0], dtype=torch.float32), probs.sum(dim=1))\n\nlogits[0], probs[0]  # Just to compare some values\n\n(tensor([-0.1117, -0.4966,  0.1631, -0.8817,  0.0539,  0.6684, -0.0597, -0.4675,\n         -0.2153,  0.8840, -0.7584, -0.3689, -0.3424, -1.4020,  0.3206, -1.0219,\n          0.7988, -0.0923, -0.7049, -1.6024,  0.2891,  0.4899, -0.3853, -0.7120,\n         -0.1706, -1.4594,  0.2207,  0.2463, -1.3248]),\n tensor([0.0336, 0.0229, 0.0443, 0.0156, 0.0397, 0.0734, 0.0354, 0.0236, 0.0303,\n         0.0910, 0.0176, 0.0260, 0.0267, 0.0093, 0.0518, 0.0135, 0.0836, 0.0343,\n         0.0186, 0.0076, 0.0502, 0.0614, 0.0256, 0.0185, 0.0317, 0.0087, 0.0469,\n         0.0481, 0.0100]))"
  },
  {
    "objectID": "posts/projects/fantasy_makemore/index.html#optimizing-the-neural-net",
    "href": "posts/projects/fantasy_makemore/index.html#optimizing-the-neural-net",
    "title": "Exploring Karpathy’s Makemore Concepts Using Fantasy DnD Names: Part 1",
    "section": "Optimizing the Neural Net",
    "text": "Optimizing the Neural Net\nWith probabilities in hand, we can now calculate the loss and create a training loop with backpropagation to update the weights.\n\n\n\n\n\n\nNote\n\n\n\nWe won’t be going over backprop here, but you can read all about it in this Stanford CS231n resource. Also, you can look at a simple vectorized feedforward net I implemented through GitHub.\n\n\n\n\n\n\n\n\nEnsuring Autodiff\n\n\n\nIn order for PyTorch to automatically record and calculate gradients, we’ll need to specify requires_grad=True in the attributes of tensors involved in the model.\n\n\n\nTraining Loop\n\ng = torch.Generator().manual_seed(1234)\nW = torch.randn((len(stoi), len(stoi)), generator=g, requires_grad=True)\n\nepochs = 200\nfor i in range(epochs):\n    # Forward pass\n    x_enc = F.one_hot(X, num_classes=len(stoi)).float()\n    logits = x_enc @ W\n    counts = logits.exp()\n    probs = counts / counts.sum(dim=1, keepdim=True)\n\n    # Calculate negative log-likelihood\n    loss = -probs[torch.arange(num_X), y].log().mean()\n    print(f\"Epoch {i}, Loss: {loss.item():.4f}\")\n\n    # Backward pass\n    W.grad = None  # Zero out the gradient before backprop\n    loss.backward()  # Backprop\n\n    # Update step\n    lr = 50  # Can use a very high learning rate on our simple data\n    W.data += -lr * W.grad\n\nEpoch 0, Loss: 3.8228\nEpoch 1, Loss: 3.5151\nEpoch 2, Loss: 3.3178\nEpoch 3, Loss: 3.1808\nEpoch 4, Loss: 3.0822\nEpoch 5, Loss: 3.0082\nEpoch 6, Loss: 2.9509\nEpoch 7, Loss: 2.9054\nEpoch 8, Loss: 2.8684\nEpoch 9, Loss: 2.8378\nEpoch 10, Loss: 2.8120\nEpoch 11, Loss: 2.7899\nEpoch 12, Loss: 2.7708\nEpoch 13, Loss: 2.7541\nEpoch 14, Loss: 2.7395\nEpoch 15, Loss: 2.7265\nEpoch 16, Loss: 2.7149\nEpoch 17, Loss: 2.7045\nEpoch 18, Loss: 2.6951\nEpoch 19, Loss: 2.6867\nEpoch 20, Loss: 2.6790\nEpoch 21, Loss: 2.6719\nEpoch 22, Loss: 2.6655\nEpoch 23, Loss: 2.6596\nEpoch 24, Loss: 2.6542\nEpoch 25, Loss: 2.6491\nEpoch 26, Loss: 2.6445\nEpoch 27, Loss: 2.6402\nEpoch 28, Loss: 2.6362\nEpoch 29, Loss: 2.6324\nEpoch 30, Loss: 2.6290\nEpoch 31, Loss: 2.6257\nEpoch 32, Loss: 2.6226\nEpoch 33, Loss: 2.6198\nEpoch 34, Loss: 2.6171\nEpoch 35, Loss: 2.6145\nEpoch 36, Loss: 2.6121\nEpoch 37, Loss: 2.6098\nEpoch 38, Loss: 2.6077\nEpoch 39, Loss: 2.6056\nEpoch 40, Loss: 2.6037\nEpoch 41, Loss: 2.6019\nEpoch 42, Loss: 2.6001\nEpoch 43, Loss: 2.5984\nEpoch 44, Loss: 2.5968\nEpoch 45, Loss: 2.5953\nEpoch 46, Loss: 2.5938\nEpoch 47, Loss: 2.5924\nEpoch 48, Loss: 2.5910\nEpoch 49, Loss: 2.5897\nEpoch 50, Loss: 2.5885\nEpoch 51, Loss: 2.5873\nEpoch 52, Loss: 2.5861\nEpoch 53, Loss: 2.5850\nEpoch 54, Loss: 2.5839\nEpoch 55, Loss: 2.5829\nEpoch 56, Loss: 2.5819\nEpoch 57, Loss: 2.5809\nEpoch 58, Loss: 2.5800\nEpoch 59, Loss: 2.5791\nEpoch 60, Loss: 2.5782\nEpoch 61, Loss: 2.5773\nEpoch 62, Loss: 2.5765\nEpoch 63, Loss: 2.5757\nEpoch 64, Loss: 2.5750\nEpoch 65, Loss: 2.5742\nEpoch 66, Loss: 2.5735\nEpoch 67, Loss: 2.5728\nEpoch 68, Loss: 2.5721\nEpoch 69, Loss: 2.5714\nEpoch 70, Loss: 2.5708\nEpoch 71, Loss: 2.5701\nEpoch 72, Loss: 2.5695\nEpoch 73, Loss: 2.5689\nEpoch 74, Loss: 2.5684\nEpoch 75, Loss: 2.5678\nEpoch 76, Loss: 2.5672\nEpoch 77, Loss: 2.5667\nEpoch 78, Loss: 2.5662\nEpoch 79, Loss: 2.5657\nEpoch 80, Loss: 2.5652\nEpoch 81, Loss: 2.5647\nEpoch 82, Loss: 2.5642\nEpoch 83, Loss: 2.5638\nEpoch 84, Loss: 2.5633\nEpoch 85, Loss: 2.5629\nEpoch 86, Loss: 2.5624\nEpoch 87, Loss: 2.5620\nEpoch 88, Loss: 2.5616\nEpoch 89, Loss: 2.5612\nEpoch 90, Loss: 2.5608\nEpoch 91, Loss: 2.5604\nEpoch 92, Loss: 2.5601\nEpoch 93, Loss: 2.5597\nEpoch 94, Loss: 2.5594\nEpoch 95, Loss: 2.5590\nEpoch 96, Loss: 2.5587\nEpoch 97, Loss: 2.5583\nEpoch 98, Loss: 2.5580\nEpoch 99, Loss: 2.5577\nEpoch 100, Loss: 2.5574\nEpoch 101, Loss: 2.5571\nEpoch 102, Loss: 2.5568\nEpoch 103, Loss: 2.5565\nEpoch 104, Loss: 2.5562\nEpoch 105, Loss: 2.5559\nEpoch 106, Loss: 2.5556\nEpoch 107, Loss: 2.5553\nEpoch 108, Loss: 2.5551\nEpoch 109, Loss: 2.5548\nEpoch 110, Loss: 2.5546\nEpoch 111, Loss: 2.5543\nEpoch 112, Loss: 2.5541\nEpoch 113, Loss: 2.5538\nEpoch 114, Loss: 2.5536\nEpoch 115, Loss: 2.5534\nEpoch 116, Loss: 2.5531\nEpoch 117, Loss: 2.5529\nEpoch 118, Loss: 2.5527\nEpoch 119, Loss: 2.5525\nEpoch 120, Loss: 2.5523\nEpoch 121, Loss: 2.5520\nEpoch 122, Loss: 2.5518\nEpoch 123, Loss: 2.5516\nEpoch 124, Loss: 2.5514\nEpoch 125, Loss: 2.5512\nEpoch 126, Loss: 2.5511\nEpoch 127, Loss: 2.5509\nEpoch 128, Loss: 2.5507\nEpoch 129, Loss: 2.5505\nEpoch 130, Loss: 2.5503\nEpoch 131, Loss: 2.5501\nEpoch 132, Loss: 2.5500\nEpoch 133, Loss: 2.5498\nEpoch 134, Loss: 2.5496\nEpoch 135, Loss: 2.5495\nEpoch 136, Loss: 2.5493\nEpoch 137, Loss: 2.5492\nEpoch 138, Loss: 2.5490\nEpoch 139, Loss: 2.5488\nEpoch 140, Loss: 2.5487\nEpoch 141, Loss: 2.5485\nEpoch 142, Loss: 2.5484\nEpoch 143, Loss: 2.5482\nEpoch 144, Loss: 2.5481\nEpoch 145, Loss: 2.5480\nEpoch 146, Loss: 2.5478\nEpoch 147, Loss: 2.5477\nEpoch 148, Loss: 2.5476\nEpoch 149, Loss: 2.5474\nEpoch 150, Loss: 2.5473\nEpoch 151, Loss: 2.5472\nEpoch 152, Loss: 2.5470\nEpoch 153, Loss: 2.5469\nEpoch 154, Loss: 2.5468\nEpoch 155, Loss: 2.5467\nEpoch 156, Loss: 2.5465\nEpoch 157, Loss: 2.5464\nEpoch 158, Loss: 2.5463\nEpoch 159, Loss: 2.5462\nEpoch 160, Loss: 2.5461\nEpoch 161, Loss: 2.5460\nEpoch 162, Loss: 2.5459\nEpoch 163, Loss: 2.5457\nEpoch 164, Loss: 2.5456\nEpoch 165, Loss: 2.5455\nEpoch 166, Loss: 2.5454\nEpoch 167, Loss: 2.5453\nEpoch 168, Loss: 2.5452\nEpoch 169, Loss: 2.5451\nEpoch 170, Loss: 2.5450\nEpoch 171, Loss: 2.5449\nEpoch 172, Loss: 2.5448\nEpoch 173, Loss: 2.5447\nEpoch 174, Loss: 2.5446\nEpoch 175, Loss: 2.5445\nEpoch 176, Loss: 2.5445\nEpoch 177, Loss: 2.5444\nEpoch 178, Loss: 2.5443\nEpoch 179, Loss: 2.5442\nEpoch 180, Loss: 2.5441\nEpoch 181, Loss: 2.5440\nEpoch 182, Loss: 2.5439\nEpoch 183, Loss: 2.5438\nEpoch 184, Loss: 2.5438\nEpoch 185, Loss: 2.5437\nEpoch 186, Loss: 2.5436\nEpoch 187, Loss: 2.5435\nEpoch 188, Loss: 2.5434\nEpoch 189, Loss: 2.5433\nEpoch 190, Loss: 2.5433\nEpoch 191, Loss: 2.5432\nEpoch 192, Loss: 2.5431\nEpoch 193, Loss: 2.5430\nEpoch 194, Loss: 2.5430\nEpoch 195, Loss: 2.5429\nEpoch 196, Loss: 2.5428\nEpoch 197, Loss: 2.5427\nEpoch 198, Loss: 2.5427\nEpoch 199, Loss: 2.5426\n\n\nThe expected loss should converge to around the same point that our counting model did. Why would we want to use the neural net framework?\nA couple of points to think about from the Karpathy’s lecture:\n\nAs the number of tokens we use to predict the next token increases, the counting approach doesn’t scale. For an n-gram model, the frequency matrix will either have shape \\(29^n \\times 29\\) for a 2D tensor or increase the number of dimensions to an ND tensor. You can imagine this would get unyieldy quickly.\nAlthough the neural net took on a similar approach to counting, there are many more settings we can improve on with the neural net. As we iterate on the architecture, the way we output the logits (the forward pass) will change but everything else will remain the same."
  },
  {
    "objectID": "posts/projects/fantasy_makemore/index.html#one-hot-encoding-seems-wasteful",
    "href": "posts/projects/fantasy_makemore/index.html#one-hot-encoding-seems-wasteful",
    "title": "Exploring Karpathy’s Makemore Concepts Using Fantasy DnD Names: Part 1",
    "section": "One-Hot Encoding Seems Wasteful…",
    "text": "One-Hot Encoding Seems Wasteful…\nIn the bigram neural net, all our one-hot encoding matrix is doing is indexing the \\(i^{th}\\) row of the weight matrix when we multiply them together. We can eliminate the matrix multiplication entirely and use the indices themselves.\n\nassert torch.all((x_enc @ W)[0] == W[0]).item()\n(x_enc @ W)[0], W[0]\n\n(tensor([-3.0790, -3.1019, -3.0673, -0.0794,  0.6222,  0.0568,  0.7115, -0.1114,\n         -1.2216, -0.0568, -0.0403,  0.5561,  0.2973,  0.6091, -0.0517,  0.3182,\n          0.7183, -0.3287, -0.7147, -0.2213,  0.8389,  0.8457,  0.8208, -0.7500,\n          0.5668, -0.2676, -1.3057, -0.7654, -0.2419],\n        grad_fn=&lt;SelectBackward0&gt;),\n tensor([-3.0790, -3.1019, -3.0673, -0.0794,  0.6222,  0.0568,  0.7115, -0.1114,\n         -1.2216, -0.0568, -0.0403,  0.5561,  0.2973,  0.6091, -0.0517,  0.3182,\n          0.7183, -0.3287, -0.7147, -0.2213,  0.8389,  0.8457,  0.8208, -0.7500,\n          0.5668, -0.2676, -1.3057, -0.7654, -0.2419],\n        grad_fn=&lt;SelectBackward0&gt;))\n\n\nFor the trigram model, our one-hot encoding would have looked a little different. We would have had two \\(1 \\times 29\\) arrays after one-hot encoding a single observation. To represent the bigram properly, we could flatten the input by reshaping it to be \\(1 \\times (29 \\times 2)\\).\n\nx_enc = F.one_hot(X_tri, num_classes=len(stoi))\nx_enc.reshape(-1, len(stoi) * 2)[\n    0\n]  # Example of the first one-hot encoded input for trigram\n\ntensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\n\nHowever, to directly index into the weight matrix we’ll instead add the corresponding weights of the two input one-hot vectors.\n\ng = torch.Generator().manual_seed(1234)\nW_tri = torch.randn(len(stoi) * 2, len(stoi), generator=g, requires_grad=True)\n\n(\n    W_tri[X_tri[:, 0]] + W_tri[X_tri[:, 1] + len(stoi)]\n)  # If we were to work out the math this should be the same as the one-hot matrix multiplication\n\ntensor([[-1.3989,  0.3646, -0.5769,  ...,  1.1644,  0.2497, -1.0169],\n        [-2.4248,  2.7207, -0.9457,  ...,  1.2563, -1.0174, -2.0139],\n        [ 1.2271, -1.3894, -0.8112,  ..., -0.5733,  0.3006, -5.3611],\n        ...,\n        [-0.4182, -0.4081, -0.4229,  ...,  3.3730,  0.9064,  0.9942],\n        [ 1.4163,  2.6578,  2.3376,  ...,  1.6067,  0.1897,  0.2689],\n        [ 1.2769,  0.4757, -1.4108,  ...,  0.9325, -1.8790,  0.2165]],\n       grad_fn=&lt;AddBackward0&gt;)"
  },
  {
    "objectID": "posts/projects/fantasy_makemore/index.html#cross-entropy-loss",
    "href": "posts/projects/fantasy_makemore/index.html#cross-entropy-loss",
    "title": "Exploring Karpathy’s Makemore Concepts Using Fantasy DnD Names: Part 1",
    "section": "Cross Entropy Loss",
    "text": "Cross Entropy Loss\nPyTorch has a function F.cross_entropy() which calculates the negative log-likelihood given the logits and target tensors.\nFrom the docs:\n\nCompute the cross entropy loss between input logits and target.\ninput (Tensor): Predicted unnormalized logits; see Shape section below for supported shapes\ntarget (Tensor): Ground truth class indices or class probabilities; see Shape section below for supported shapes.\n\nEssentially, this function expects our raw logits since it will softmax them for us and calculate the negative log-likelihood in a much more computationally efficient manner (leather boots -&gt; boots of swiftness upgrade).\nThis along with the will replace the bulk of our forward pass."
  },
  {
    "objectID": "posts/projects/fantasy_makemore/index.html#trigram-model-1",
    "href": "posts/projects/fantasy_makemore/index.html#trigram-model-1",
    "title": "Exploring Karpathy’s Makemore Concepts Using Fantasy DnD Names: Part 1",
    "section": "Trigram Model",
    "text": "Trigram Model\n\ng = torch.Generator().manual_seed(1234)\nW_tri = torch.randn(len(stoi) * 2, len(stoi), generator=g, requires_grad=True)\n\nepochs = 200\n\nfor i in range(epochs):\n    # Forward pass (prev)\n    # x_enc = F.one_hot(X_tri, num_classes=len(stoi)).float()\n    # logits = x_enc @ W_tri\n    # counts = logits.exp()\n    # probs = counts / counts.sum(dim=1, keepdim=True)\n\n    # Forward pass (new)\n    logits = W_tri[X_tri[:, 0]] + W_tri[X_tri[:, 1] + len(stoi)]\n\n    # Calculate negative log-likelihood (prev)\n    # loss = -probs[torch.arange(num_X), y_tri].log().mean()\n\n    # Calculate negative log-likelihood (new)\n    loss = F.cross_entropy(logits, y_tri)\n\n    # Print metrics every 10 epochs\n    if i % 10 == 0:\n        print(f\"Epoch {i}, Loss: {loss.item():.4f}\")\n\n    # Backward pass\n    W_tri.grad = None\n    loss.backward()\n\n    # Update step\n    lr = 50\n    W_tri.data += -lr * W_tri.grad\n\nEpoch 0, Loss: 4.2444\nEpoch 10, Loss: 2.6864\nEpoch 20, Loss: 2.4972\nEpoch 30, Loss: 2.4244\nEpoch 40, Loss: 2.3866\nEpoch 50, Loss: 2.3638\nEpoch 60, Loss: 2.3487\nEpoch 70, Loss: 2.3382\nEpoch 80, Loss: 2.3303\nEpoch 90, Loss: 2.3243\nEpoch 100, Loss: 2.3196\nEpoch 110, Loss: 2.3158\nEpoch 120, Loss: 2.3126\nEpoch 130, Loss: 2.3100\nEpoch 140, Loss: 2.3077\nEpoch 150, Loss: 2.3058\nEpoch 160, Loss: 2.3041\nEpoch 170, Loss: 2.3026\nEpoch 180, Loss: 2.3013\nEpoch 190, Loss: 2.3001"
  },
  {
    "objectID": "posts/projects/fantasy_makemore/index.html#generating-characters-from-neural-net",
    "href": "posts/projects/fantasy_makemore/index.html#generating-characters-from-neural-net",
    "title": "Exploring Karpathy’s Makemore Concepts Using Fantasy DnD Names: Part 1",
    "section": "Generating Characters from Neural Net",
    "text": "Generating Characters from Neural Net\nFinally, let’s see how we can generate characters from the neural net. A new band of adventurers await…\n\ng = torch.Generator().manual_seed(1234)\n\nfor i in range(10):\n    out = [\"y\"]\n    ix1, ix2 = 0, 27\n    while True:\n        logits = W_tri[[ix1]] + W_tri[[ix2 + len(stoi)]]\n        counts = logits.exp()\n        p = counts / counts.sum(dim=1, keepdim=True)\n\n        ix1 = ix2\n        ix2 = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n        if ix2 == 0:\n            break\n        out.append(itos[ix2])\n    print(\"\".join(out))\n\nylunorlelatorele feizminil'rar ho'gorlavzquvh drizzan\nyr dorhimluan maelolaar\nym\nymaanichaunduvrth\nyl\nym\nymwelvir dushont\nylym cl\nyr\nymden kryl\n\n\nThe generated names are a bit different than the counting model names. Our loss was also worse, but we’ll be undergoing a big architecture change soon which can capture the character relationships better. Note that we didn’t use a train/val/test split so our experiments could be overfitting, underfitting–it’s hard to tell. This post is just a demonstration of how we can model our names. We’ll create our splits next time.\nWith that, we’ll wrap up this session of our glorious campaign. It’s a long one, but we covered plenty of valuable information for our character creation needs. Stay tuned for Part 2!"
  },
  {
    "objectID": "posts/projects/artwork_classifier/index.html",
    "href": "posts/projects/artwork_classifier/index.html",
    "title": "Classifying Artwork Types With Fastai",
    "section": "",
    "text": "I’ve always been interested in how machine learning can apply to visual arts. In particular, computer vision feels like the most direct application to art since we can build programs to classify, modify, and even generate visuals in an artistic fashion!\nI’m by no means a deep learning aficionado but it’s exciting to tinker around with the different tools and concepts I have at my disposal. In this notebook, we will be exploring how to build out an image classification model which classifies artwork based on their different types.\nWe’ll be using the fastai framework to speed up the development process and utilize modern deep learning techniques. Along the way, I will also be covering several different concepts that I have found useful in implementing vision models. I assume some level of familiarity with machine learning concepts such as epochs and mini-batch gradient descent. And ultimately, my hope is for this notebook to become the start of projects that inspire and help other deep learning practitioners along their own journeys!"
  },
  {
    "objectID": "posts/projects/artwork_classifier/index.html#dataloader-function",
    "href": "posts/projects/artwork_classifier/index.html#dataloader-function",
    "title": "Classifying Artwork Types With Fastai",
    "section": "4.1 Dataloader Function",
    "text": "4.1 Dataloader Function\n\n\nCode\ndef get_dls(bs: int, size: int, valid_pct=0.2, seed=1234) -&gt; DataLoaders:\n    \"\"\"\n    Creates a dataloader for an image recognition task with specified batch size and image size.\n\n    # Parameters:\n    -   bs (int): Batch size.\n    -   size (int): Final size of individual images.\n\n    # Returns:\n    -   DataLoaders: A dataloader that batches our image data with specific image and batch size.\n    \"\"\"\n    db = DataBlock(\n        blocks=(ImageBlock(), CategoryBlock()),\n        get_items=get_image_files,\n        splitter=RandomSplitter(valid_pct=valid_pct, seed=seed), \n        get_y=parent_label,\n        item_tfms=Resize(300),\n        batch_tfms=aug_transforms(size=size, min_scale=0.75)\n    )\n    return db.dataloaders(data_path / 'training_set', bs=bs, seed=1234)\n\n\nWhen we create our dataloader, we can verify that our splitter is working as we expect it to.\n\n\nCode\nset_seed(SEED, reproducible=True)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\ndls = get_dls(64, 128, seed=SEED)\n\nlen(dls.train_ds), len(dls.valid_ds)\n\n\n(6177, 1544)\n\n\nNow, let’s check out what images we have to work with using the show_batch function!\n\n\nCode\n# Show an example batch of images\ndls.show_batch()"
  },
  {
    "objectID": "posts/projects/artwork_classifier/index.html#learning-rate-finder",
    "href": "posts/projects/artwork_classifier/index.html#learning-rate-finder",
    "title": "Classifying Artwork Types With Fastai",
    "section": "5.1 Learning Rate Finder",
    "text": "5.1 Learning Rate Finder\nFor our model, we first need to select a good learning rate to start our training. Fastai implements a learning rate finder method Learner.lr_find based on Leslie Smith’s paper called Cyclical Learning Rates for Training Neural Networks.\nSmith describes his approach as the following:\n\nThere is a simple way to estimate reasonable minimum and maximum boundary values with one training run of the network for a few epochs. It is a “LR range test”; run your model for several epochs while letting the learning rate increase linearly between low and high LR values.\n\nEssentially, we check our training losses as we grow our learning rate to get an idea of what learning rate performs well.\nFastai takes a slightly different approach: The model is test-trained with learning rates that grow exponentially from a low learning rate to a higher learning rate across a number of mini-batches. The process continues until we find a learning rate where the loss begins to diverge and increases significantly.\nWhat this boils down to is that in the fastai version we don’t necessarily need to run for multiple epochs to get an optimal learning rate as we are training for a set number of mini-batches.\nConveniently, lr_find gives us the loss vs. learning rate plot and also enables us to directly extract learning rates that could be optimal based off different suggestion functions. For example, we can use steep which gets the learning rate with the steepest slope.\n\n\nCode\nset_seed(SEED, reproducible=True)\nlr_steep = learn.lr_find(suggest_funcs=steep)\n\n\n\nLog plot of loss vs. learning rate"
  },
  {
    "objectID": "posts/projects/artwork_classifier/index.html#one-cycle-idea",
    "href": "posts/projects/artwork_classifier/index.html#one-cycle-idea",
    "title": "Classifying Artwork Types With Fastai",
    "section": "5.2 One-cycle Idea",
    "text": "5.2 One-cycle Idea\nWe can now start training our model! Using the fit_one_cycle method we are actually training our model with what is called the one-cycle policy, an idea that again comes from Leslie Smith and changes our learning rate over the course of training. I won’t be going into close detail on how the 1cycle policy is implemented in the scope of this article.\nAt a high level, we are starting at some initial learning rate, linearly increasing our learning rate after every batch up to a maximum learning rate, and from the maximum learning rate down to some minimum learning rate several magnitudes lower than our initial learning rate.\nThe idea behind this all is to warm up our training with a low learning rate and use the high learning rate to help find minimums in our loss function that are flatter, allowing the model to generalize better.\nDuring the last segment of training, the descending learning rates help the optimizer avoid skipping over a steeper loss within the flatter areas. This process allows our model to converge faster and consequently achieve better results with lower iterations than traditional training methods. Smith calls this occurrence super-convergence.\n\n\nCode\ndef plot_example_cycle(x_min, x_max, y_min, y_max, padding):\n    values = 100\n    x = np.linspace(x_min, x_max, values + padding)\n\n    y1 = np.linspace(y_min, y_max, values // 2, endpoint=False)\n    y2 = np.linspace(y_max, y_min, values // 2, endpoint=False)\n    padded_values = np.linspace(y_min, y_min * 1e-2, padding)\n\n    y = np.concatenate((y1, y2, padded_values))\n\n    plt.plot(x, y)\n    plt.xlabel('Epochs')\n    plt.ylabel('Learning rate')\n    plt.show()\n\n\n\n\nCode\nplot_example_cycle(0, 40, 0.001, 0.01, 15)\n\n\n\n\n\n\n\n\nFigure 1: Example representation of a linear 1cycle learning rate schedule.\n\n\n\n\n\n\nWith transfer learning, our model has additional new layers that we can train for the problem we are trying to solve. At the start of training, we may not want to completely readjust the weights in the model’s previously learned layers because they account for high-level details of the image like line and shape.\nIn a pretrained model, the previous trained layers start out frozen, meaning their weights aren’t updated during training unless we unfreeze them. Later, we will unfreeze all the layers and use a range of learning rates to help adjust them slightly for our artwork classification.\nFastai recommends training the frozen pretrained model for a few epochs before training the full pretrained model. We’ll start with 3 epochs frozen using the learning rate we found before and then train the full model for 10 epochs. We can get an idea of how our error rates changes across training from here. Our losses along with our metrics will be useful in determining if we’ve fitted a decent model or whether we are underfitting or overfitting.\n\n\nCode\nset_seed(SEED, reproducible=True)\nlearn.fit_one_cycle(3, lr_steep.steep)\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.736086\n0.369670\n0.887953\n00:08\n\n\n1\n0.441399\n0.284378\n0.909326\n00:08\n\n\n2\n0.321099\n0.254835\n0.911269\n00:08\n\n\n\n\n\nNow we can unfreeze the layers and find a new learning rate to train on:\n\n\nCode\nset_seed(SEED, reproducible=True)\nlearn.unfreeze()\nlearn.lr_find()\n\n\n\n\n\nSuggestedLRs(valley=0.00013182566908653826)\n\n\n\n\n\n\n\n\n\n\n\nCode\nset_seed(SEED, reproducible=True)\nlearn.fit_one_cycle(10, 1e-5)\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.290775\n0.243898\n0.915155\n00:09\n\n\n1\n0.292668\n0.236558\n0.915155\n00:09\n\n\n2\n0.248140\n0.215108\n0.920984\n00:09\n\n\n3\n0.219748\n0.209778\n0.926166\n00:09\n\n\n4\n0.192568\n0.201274\n0.928109\n00:09\n\n\n5\n0.186713\n0.200779\n0.926813\n00:09\n\n\n6\n0.169397\n0.188377\n0.929404\n00:09\n\n\n7\n0.157394\n0.192167\n0.931347\n00:09\n\n\n8\n0.146395\n0.195563\n0.928109\n00:09\n\n\n9\n0.159019\n0.191388\n0.931995\n00:09"
  },
  {
    "objectID": "posts/projects/artwork_classifier/index.html#interpreting-the-loss-curves-further-training",
    "href": "posts/projects/artwork_classifier/index.html#interpreting-the-loss-curves-further-training",
    "title": "Classifying Artwork Types With Fastai",
    "section": "5.3 Interpreting the Loss Curves, Further Training?",
    "text": "5.3 Interpreting the Loss Curves, Further Training?\nAfter training our model, we will see how our training and validation losses have changed over the course of our iteration / epochs using loss curves. Loss curves won’t provide the entire story of our model but we’ll have a broad picture of how our model performs over the selected dataset and batch size. Figure 2 shows the loss curves across the training iterations.\n\n\nCode\nlearn.recorder.plot_loss()\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\nplt.title('Loss Curves Across Iterations')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: Loss curves across the model’s training iterations\n\n\n\n\n\nInitially, our validation loss starts out lower than our training loss but our training loss quickly converges as our training progresses.\nWe should note that we reach a point where our validation loss is higher than our training loss and stagnates a bit. The model is becoming overconfident in its predictions and we have to ask ourselves: are we overfitting?\nFigure 3 shows the loss curves across the training epochs in case we need to go back and retrain our model to a previous epoch.\n\n\nCode\ntrain_loss = L(learn.recorder.values).itemgot(0)\nvalid_loss = L(learn.recorder.values).itemgot(1)\n\nplt.plot(train_loss)\nplt.plot(valid_loss)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Loss Curves Across Epochs')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: Loss curves across training epochs"
  },
  {
    "objectID": "posts/projects/artwork_classifier/index.html#overfitting",
    "href": "posts/projects/artwork_classifier/index.html#overfitting",
    "title": "Classifying Artwork Types With Fastai",
    "section": "5.4 Overfitting",
    "text": "5.4 Overfitting\nOverfitting occurs when we have trained for too long and the model begins to “memorize” the training data while failing to generalize well to new data.\nWhen we look at our losses visually, we can see that there is an emerging gap between our training and validation loss and the latter seems to stagnate. How big of a gap is too big? It’s hard to say because it depends on a variety of factors like the scale of the dataset. Rather, we should look for general stability in our validation loss throughout training. If our validation loss begins to increase significantly, then we might begin to worry that we are overfitting to our training data too much.\nWe could stop early, but with one-cycle training that may not be a good idea because we may not allow our learning rate to reach the small values that would benefit our training.\nOne key takeaway from fastai is that we should be checking to see if our performance metrics are getting significantly worse to decide if the model is overfitting. It’s not enough to view the losses alone.\nFigure 4 shows the accuracy across the training epochs.\n\n\nCode\naccuracy_metric = L(learn.recorder.values).itemgot(2)\nhighest_accuracy_epoch = np.argmax(accuracy_metric)\nprint(f'Highest accuracy during last run: {highest_accuracy_epoch}')\n\nplt.xlabel('Accuracy')\nplt.ylabel('Epoch')\nplt.title('Accuracy Across Epochs')\nplt.xticks(np.arange(0, len(accuracy_metric) + 1, 5))\n\nplt.plot(accuracy_metric)\nplt.scatter(highest_accuracy_epoch, accuracy_metric[highest_accuracy_epoch], color='orange')\nplt.show()\n\n\nHighest accuracy during last run: 9\n\n\n\n\n\n\n\n\nFigure 4: Accuracy across the training epochs\n\n\n\n\n\nOur highest accuracy is at epoch 9 but the accuracy appears to be flattening out towards the end. If we take into account both our validation loss and metric, then we aren’t too concerned about overfitting. Fortunately, we do have a few options in the case of severe overfitting! We could introduce weight decay or even rerun our model with a lower number of epochs and adjust from there. In this case, we will add some weight decay using discriminative learning rates.\nWe should keep in mind that our performance metric is what ultimately matters in practice.\nAs Jeremy Howard states:\n\n“In the end what matters is your accuracy, or more generally your chosen metrics, not the loss. The loss is just the function we’ve given the computer to help us to optimize.”\n“Remember, it’s not just that we’re looking for the validation loss to get worse, but the actual metrics. Your validation loss will first get worse during training because the model gets overconfident, and only later will get worse because it is incorrectly memorizing the data. We only care in practice about the latter issue. Remember, our loss function is just something that we use to allow our optimizer to have something it can differentiate and optimize; it’s not actually the thing we care about in practice.”\n\nTo really evaluate our model’s performance, we would run our model on a representative test data set that it has never seen before. This will allow us to get a more honest assessment of how our model is doing and we don’t want to report accuracy on our training model alone. Once we’ve decided on a model, we’ll look at how we can use the images we didn’t use for training as a test set.\n\n\nCode\nset_seed(SEED, reproducible=True)\n\n# Reinitialize our model to restart training\nlearn2 = vision_learner(dls, resnet50, metrics=accuracy, wd=0.1)\n\n\n\n\nCode\nset_seed(SEED, reproducible=True)\nlr_steep = learn2.lr_find(suggest_funcs=steep, show_plot=False)\n\n\n\n\n\n\n\nCode\nset_seed(SEED, reproducible=True)\nlearn2.fit_one_cycle(3, lr_steep.steep)\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.737313\n0.343379\n0.890544\n00:08\n\n\n1\n0.418140\n0.294040\n0.908679\n00:08\n\n\n2\n0.324219\n0.272586\n0.914508\n00:08\n\n\n\n\n\n\n\nCode\nset_seed(SEED, reproducible=True)\nlearn2.unfreeze()\nlearn2.lr_find()\n\n\n\n\n\nSuggestedLRs(valley=6.30957365501672e-05)\n\n\n\n\n\n\n\n\n\n\n\nCode\nset_seed(SEED, reproducible=True)\nlearn2.fit_one_cycle(10, lr_max=slice(1e-6, 1e-4))\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.284886\n0.262790\n0.918394\n00:09\n\n\n1\n0.272633\n0.263987\n0.917746\n00:09\n\n\n2\n0.253378\n0.242250\n0.922280\n00:10\n\n\n3\n0.223389\n0.225795\n0.930052\n00:09\n\n\n4\n0.204689\n0.231059\n0.920984\n00:09\n\n\n5\n0.170040\n0.227536\n0.925518\n00:10\n\n\n6\n0.153856\n0.218628\n0.928756\n00:09\n\n\n7\n0.148347\n0.223874\n0.927461\n00:09\n\n\n8\n0.140296\n0.223210\n0.930699\n00:09\n\n\n9\n0.136215\n0.218119\n0.930699\n00:10\n\n\n\n\n\nOur accuracy is about the same, but both validation losses seemed to stabilize. The validation loss and accuracy are better in our first model, so we will move forward with it for now.\n\n\nCode\nlearn2.recorder.plot_loss()\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\nplt.title('Loss Curves Across Iterations')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5: Loss curves across the model’s training iterations\n\n\n\n\n\n\n\nCode\ntrain_loss = L(learn2.recorder.values).itemgot(0)\nvalid_loss = L(learn2.recorder.values).itemgot(1)\n\nplt.plot(train_loss)\nplt.plot(valid_loss)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Loss Curves Across Epochs')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 6: Loss curves across training epochs\n\n\n\n\n\n\n\nCode\naccuracy_metric = L(learn2.recorder.values).itemgot(2)\nhighest_accuracy_epoch = np.argmax(accuracy_metric)\nprint(f'Highest accuracy during last run: {highest_accuracy_epoch}')\n\nplt.xlabel('Accuracy')\nplt.ylabel('Epoch')\nplt.title('Accuracy Across Epochs')\nplt.xticks(np.arange(0, len(accuracy_metric) + 1, 5))\n\nplt.plot(accuracy_metric)\nplt.scatter(highest_accuracy_epoch, accuracy_metric[highest_accuracy_epoch])\nplt.show()\n\n\nHighest accuracy during last run: 8\n\n\n\n\n\n\n\n\nFigure 7: Accuracy across the training epochs\n\n\n\n\n\nWe can use a confusion matrix to see the number of correctly classified and misclassified images on our training set. Figure 8 displays the confusion matrix for the first model we fine-tuned.\n\n\nCode\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n(a) Confusion matrix on the training set\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure 8\n\n\n\n\nWe can also display a report with common classification metrics like precision, recall, and the F1-score. For this project, we are mainly focused on the F1-score.\nThe weighted F1-score is the the mean of all the class F1 scores while taking into account the number of occurrences in each class. In general, we are aiming for a balance between precision and recall on a scale between 0 and 1. Our model achieved a weighted F1-score of 0.930 on the training set.\n\n\nCode\ninterp.print_classification_report()\n\n\n\n\n\n              precision    recall  f1-score   support\n\n    drawings       0.83      0.75      0.79       208\n   engraving       0.78      0.86      0.82       161\n iconography       0.97      1.00      0.98       419\n    painting       0.98      0.94      0.96       410\n   sculpture       0.96      0.99      0.97       346\n\n    accuracy                           0.93      1544\n   macro avg       0.90      0.91      0.90      1544\nweighted avg       0.93      0.93      0.93      1544\n\n\n\nFigure 9 shows the top 10 losses of training images predicted by our model. We can use this to get a visual idea of what kinds of images our model might be misclassifying.\nIn particular, from both the classification report and the top losses our model seems to misclassify engravings and drawings more than other types of art. We also see some confusion between iconography and paintings. Maybe not all images are standardized and there are definitely tough cases where the line between painting, drawing, and iconography are blurry.\n\n\nCode\ninterp.plot_top_losses(10)\n\n\n\n\n\n\n\n\n\n\n(a) Top 10 losses of images in the training set\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure 9\n\n\n\n\nIt’s fascinating that we are able to classify images at all with these results with a bit of conceptual understanding and a few lines of code! However, we still need to evaluate on the test set for reporting purposes."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cracking Data with Yang",
    "section": "",
    "text": "Exploring Karpathy’s Makemore Concepts Using Fantasy DnD Names: Part 1\n\n\n\n\n\n\nproject\n\n\n\n\n\n\n\n\n\nJun 22, 2024\n\n\nYang Chen\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Artwork Types With Fastai\n\n\n\n\n\n\nproject\n\n\nmachine learning\n\n\ncomputer vision\n\n\n\n\n\n\n\n\n\nSep 27, 2023\n\n\nYang Chen\n\n\n\n\n\n\nNo matching items"
  }
]